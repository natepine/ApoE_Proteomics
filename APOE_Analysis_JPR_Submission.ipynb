{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Drive Mount and Package Import"
      ],
      "metadata": {
        "id": "L7mhw-WoTZRW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6GIN7zQTWMN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install bioinfokit\n",
        "!pip install seaborn\n",
        "!pip install seaborn[stats]\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from pandas.core.algorithms import duplicated\n",
        "from pandas.errors import AccessorRegistrationWarning\n",
        "from scipy import stats as st\n",
        "from scipy.stats import ttest_ind_from_stats\n",
        "from scipy.stats import combine_pvalues\n",
        "from bioinfokit import analys, visuz\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from numpy.ma.core import mean\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import statistics\n",
        "from sklearn.impute import KNNImputer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Global Variables\n",
        "significance_cutoff = np.log10(0.05)*-10\n",
        "change_cutoff = np.log2(1)\n",
        "FC_sex = 'Male-Female'\n",
        "ontologyDB_sex = 'Male-Female'\n",
        "comparison = None"
      ],
      "metadata": {
        "id": "zjvPCgGNTira"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Scaling functions\n",
        "def auto_scaling(df):\n",
        "  df_mean = np.mean(df[f'{comparison}_Fold_change'])\n",
        "  df_std = np.std(df[f'{comparison}_Fold_change'])\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] - df_mean\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] / df_std\n",
        "  return(df)\n",
        "\n",
        "def range_scaling(df):\n",
        "  df_min = np.min(df)\n",
        "  df_max = np.max(df)\n",
        "  df_mean = np.mean(df)\n",
        "  df = df - df_mean\n",
        "  df = df / (df_max - df_min)\n",
        "  return(df)\n",
        "\n",
        "def pareto_scaling(df):\n",
        "  df_mean = np.mean(df[f'{comparison}_Fold_change'])\n",
        "  df_std = np.std(df[f'{comparison}_Fold_change'])\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] - df_mean\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] /  np.sqrt(df_std)\n",
        "  return(df)\n",
        "\n",
        "def vast_scaling(df):\n",
        "  df_mean = np.mean(df[f'{comparison}_Fold_change'])\n",
        "  df_std = np.std(df[f'{comparison}_Fold_change'])\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] - df_mean\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] / (df_std)\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] * (df_mean/df_std)\n",
        "  return(df)\n",
        "\n",
        "def level_scaling(df):\n",
        "  df_mean = np.mean(df[f'{comparison}_Fold_change'])\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] - df_mean\n",
        "  df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] / (df_mean)\n",
        "  return(df)"
      ],
      "metadata": {
        "id": "urgusRQmTqg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Section analyzes Peaks protein.csv outputs"
      ],
      "metadata": {
        "id": "OV0MXfAgUQgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization (Aguilan et Al) and stats workflow\n",
        "from numpy.core.shape_base import atleast_3d\n",
        "\n",
        "# Establish the location of the protein.csv files, and create a list of all filenames\n",
        "where_are_the_files = f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Peaks_DB_Data'\n",
        "os.chdir(where_are_the_files)\n",
        "all_file_names = [i for i in glob.glob('*.{}'.format('csv'))]\n",
        "\n",
        "# Create a dataframe for each comparison\n",
        "E2vE3_PVFC_DF = pd.DataFrame()\n",
        "E4vE3_PVFC_DF = pd.DataFrame()\n",
        "E4vE2_PVFC_DF = pd.DataFrame()\n",
        "\n",
        "# This loop uses the filename list to normalize and analyze every file individually\n",
        "for dataSet in all_file_names:\n",
        "  fileName = dataSet.strip('.csv')\n",
        "  dataDF = pd.read_csv(dataSet)\n",
        "\n",
        "  # Read CSV, set the 'Accession' as the index, filter out proteins with <2 unique peptides, and filter out non-area data.\n",
        "  dataDF = dataDF.set_index('Accession')\n",
        "  dataDF = dataDF.loc[(dataDF['#Unique'] >= 2),]\n",
        "  dataDF = dataDF.loc[dataDF['Top'] == True,]\n",
        "  dataDF = dataDF.filter(regex='Area')\n",
        "\n",
        "  # Replace 0 with NaN, create allele specific dataframes, count non-zero values, remove proteins with >1 missing value\n",
        "  dataDF = dataDF.replace(np.NaN,0)\n",
        "  dataDF['nonZero_Count'] = (dataDF != 0).astype(int).sum(axis=1)\n",
        "\n",
        "  # Create a genotype specific dataframe for the purpose of filtering out proteins with (n-1) valid values in each genotype sample set\n",
        "  E2_DF = dataDF.filter(regex='A2').copy()\n",
        "  E3_DF = dataDF.filter(regex='A3').copy()\n",
        "  E4_DF = dataDF.filter(regex='A4').copy()\n",
        "\n",
        "  # Calculate number of samples (columns) in each condition\n",
        "  cols_E2_DF = len(E2_DF.axes[1])\n",
        "  cols_E3_DF = len(E3_DF.axes[1])\n",
        "  cols_E4_DF = len(E4_DF.axes[1])\n",
        "\n",
        "  # Calculate the number of non-zero values in each condition\n",
        "  E2_DF['E2_nonZero_Count'] = (E2_DF != 0).astype(int).sum(axis=1)\n",
        "  E3_DF['E3_nonZero_Count'] = (E3_DF != 0).astype(int).sum(axis=1)\n",
        "  E4_DF['E4_nonZero_Count'] = (E4_DF != 0).astype(int).sum(axis=1)\n",
        "\n",
        "  # Remove any proteins with More than n-1 nonZeros or NANs\n",
        "  E2_DF = E2_DF.loc[(E2_DF['E2_nonZero_Count'] >= (cols_E2_DF-1)),]\n",
        "  E3_DF = E3_DF.loc[(E3_DF['E3_nonZero_Count'] >= (cols_E3_DF-1)),]\n",
        "  E4_DF = E4_DF.loc[(E4_DF['E4_nonZero_Count'] >= (cols_E4_DF-1)),]\n",
        "\n",
        "  # Create a comparison specific dataframes for analysis, delete any NAN of non-Area columns\n",
        "  E2vE3_comparison_DF = pd.concat([E2_DF,E3_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "  E4vE3_comparison_DF = pd.concat([E4_DF,E3_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "  E4vE2_comparison_DF = pd.concat([E4_DF,E2_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "\n",
        "  # Create a dataframe with all genotypes\n",
        "  E4vE3vE2_comparison_DF = pd.concat([E4_DF,E3_DF,E2_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "  E4vE3vE2_comparison_DF = E4vE3vE2_comparison_DF.replace(0,np.NaN)\n",
        "\n",
        "  # This function returns the normalized dataset\n",
        "  def slope_normalize(input_df):\n",
        "    # Creat allele comparison dataframes, log2 normalize and replace -inf values with NaN.\n",
        "    df_to_normalize = input_df.copy()\n",
        "    df_to_normalize = np.log2(df_to_normalize)\n",
        "    df_to_normalize = df_to_normalize.replace(-np.inf, np.NaN)\n",
        "\n",
        "    # Normalize Data by the Average\n",
        "    for sample in df_to_normalize.columns:\n",
        "      df_to_normalize[sample] = df_to_normalize[sample] - np.mean(df_to_normalize[sample])\n",
        "\n",
        "    # Normalize Data by the Slope\n",
        "    df_to_normalize['protein_avg'] = np.mean(df_to_normalize,axis=1)\n",
        "    for sample in df_to_normalize.columns:\n",
        "      x= df_to_normalize['protein_avg']\n",
        "      y= df_to_normalize[sample]\n",
        "      mask = ~np.isnan(x) & ~np.isnan(y)\n",
        "      slope = st.linregress(x[mask], y[mask])[0]\n",
        "      df_to_normalize[sample] = df_to_normalize[sample]/slope\n",
        "    normalized_DF = df_to_normalize.filter(regex='Area')\n",
        "\n",
        "    # Impute missing values using KNN imputer\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    normalized_DF = pd.DataFrame(imputer.fit_transform(normalized_DF),columns=normalized_DF.columns,index=normalized_DF.index)\n",
        "    x = normalized_DF.plot.density(figsize=(10, 10))\n",
        "    return(normalized_DF)\n",
        "\n",
        "  # use the normalization function to normalize the dataframe with the genotypes\n",
        "  E4vE3vE2_norm_DF = slope_normalize(E4vE3vE2_comparison_DF)\n",
        "\n",
        "  # Create normalized dataframes for each comparison\n",
        "  E2vE3_norm_DF = E4vE3vE2_norm_DF.filter(regex='A2|A3').copy()\n",
        "  E4vE3_norm_DF = E4vE3vE2_norm_DF.filter(regex='A4|A3').copy()\n",
        "  E4vE2_norm_DF = E4vE3vE2_norm_DF.filter(regex='A4|A2').copy()\n",
        "\n",
        "  # Store the normalized comparisons as CSVs\n",
        "  E2vE3_norm_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Normalized_Peaks_Data/{fileName}_E2vE3_Normalized_DataSet.csv',index=True)\n",
        "  E4vE3_norm_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Normalized_Peaks_Data/{fileName}_E4vE3_Normalized_DataSet.csv',index=True)\n",
        "  E4vE2_norm_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Normalized_Peaks_Data/{fileName}_E4vE2_Normalized_DataSet.csv',index=True)\n",
        "\n",
        "  # This function tests the datasets for equality of variance using an F-test; based on the results of the F-test, it will calculate statistical significance for each comparison; Finally, it will calcualte the Fold change\n",
        "  def stats_calc(input_df,comparison=None):\n",
        "    if comparison == 'E2vE3':\n",
        "      allele_a = 'A3'\n",
        "      allele_b = 'A2'\n",
        "    elif comparison == 'E4vE3':\n",
        "      allele_a = 'A3'\n",
        "      allele_b = 'A4'\n",
        "    elif comparison == 'E4vE2':\n",
        "      allele_a = 'A2'\n",
        "      allele_b = 'A4'\n",
        "\n",
        "    # Create genotype specific dataframes\n",
        "    comparison_DF_a = input_df.filter(regex=allele_a).copy()\n",
        "    comparison_DF_b = input_df.filter(regex=allele_b).copy()\n",
        "\n",
        "    # Create dictionaries to store PVs and FCs\n",
        "    comparison_p_val_dict = {}\n",
        "    comparison_FC_dict = {}\n",
        "\n",
        "    for protein in input_df.index.to_list():\n",
        "      #ApoEb vs ApoEa P-value and FC\n",
        "      a = comparison_DF_a.loc[protein,]\n",
        "      b = comparison_DF_b.loc[protein,]\n",
        "\n",
        "      # For each protein in the dataset, use the f-test to determine if variance is equal between both populations;\n",
        "      variance_a = np.var(a, ddof=1)\n",
        "      variance_b = np.var(b, ddof=1)\n",
        "\n",
        "      if variance_a > variance_b:\n",
        "        f_value = variance_a / variance_b\n",
        "      else:\n",
        "        f_value = variance_b / variance_a\n",
        "\n",
        "      df_a = len(a) - 1\n",
        "      df_b = len(b) - 1\n",
        "\n",
        "      if variance_a > variance_b:\n",
        "        f_PV =1 - st.f.cdf(f_value, df_a, df_b)\n",
        "      else:\n",
        "        f_PV = st.f.cdf(f_value, df_b, df_a)\n",
        "\n",
        "      # Use the results of the f-test to determine what t-test to use for to calculate PV; Calculate PV and store it in PV dictionary\n",
        "      if f_PV > 0.05:\n",
        "        comparison_p_val = st.ttest_ind(a,b, equal_var=True, alternative='two-sided',nan_policy='omit')[1]\n",
        "        comparison_p_val_dict[protein] = comparison_p_val\n",
        "\n",
        "      elif f_PV < 0.05:\n",
        "        comparison_p_val = st.ttest_ind(a,b, equal_var=False, alternative='two-sided',nan_policy='omit')[1]\n",
        "        comparison_p_val_dict[protein] = comparison_p_val\n",
        "\n",
        "      # Calculate FC and store it in FC dictionary\n",
        "      comparison_FC = (np.mean(b)) - (np.mean(a))\n",
        "      comparison_FC_dict[protein] = comparison_FC\n",
        "\n",
        "    # Convert PV and FC dictionaries into dataframes\n",
        "    comparison_df_PV = pd.DataFrame.from_dict(comparison_p_val_dict.items()).rename(columns={0 : 'Accession',1 : f'{comparison}_PV'}).set_index('Accession')\n",
        "    comparison_df_FC = pd.DataFrame.from_dict(comparison_FC_dict.items()).rename(columns={0 : 'Accession',1 : f'{comparison}_FC'}).set_index('Accession')\n",
        "\n",
        "    comparison_df_PV = pd.melt(comparison_df_PV,ignore_index=False).rename(columns={'value' : 'P_val'})\n",
        "    comparison_df_PV = comparison_df_PV.drop(columns='variable')\n",
        "\n",
        "    comparison_df_FC = pd.melt(comparison_df_FC,ignore_index=False).rename(columns={'value' : 'Fold_change'})\n",
        "\n",
        "    # Create a dataframe that contains both the PC and the FC\n",
        "    comparison_PVFC = pd.concat([comparison_df_PV,comparison_df_FC],axis=1)\n",
        "    comparison_PVFC['-log10(PV)'] = np.log10(comparison_PVFC['P_val'])*-10\n",
        "    return(comparison_PVFC)\n",
        "\n",
        "  # Use the stats_calc() function to calculate the PV and FC for each comparison\n",
        "  E2vE3_DF = stats_calc(E2vE3_norm_DF,comparison='E2vE3')\n",
        "  E4vE3_DF = stats_calc(E4vE3_norm_DF,comparison='E4vE3')\n",
        "  E4vE2_DF = stats_calc(E4vE2_norm_DF,comparison='E4vE2')\n",
        "\n",
        "  # Store the PV and FC calculations into a single dataframe that contains the PV and FC from all datasets\n",
        "  E2vE3_PVFC_DF = pd.concat([E2vE3_PVFC_DF,E2vE3_DF])\n",
        "  E4vE3_PVFC_DF = pd.concat([E4vE3_PVFC_DF,E4vE3_DF])\n",
        "  E4vE2_PVFC_DF = pd.concat([E4vE2_PVFC_DF,E4vE2_DF])\n",
        "\n",
        "  # This function Calculates the benjamini hochberg P-value correction for the DataSet\n",
        "  def performBH_correction(input_df):\n",
        "    input_df['Benjamini_Hochberg_pval'] = None\n",
        "    input_df = input_df.reset_index()\n",
        "    #sort cleaned_df by pvalue jc mod\n",
        "    input_df = input_df.sort_values(by='P_val')\n",
        "    input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "    #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "    total_rows = len(input_df.index)\n",
        "    for row in input_df.itertuples():\n",
        "      BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "      input_df.at[row.Index, 'Benjamini_Hochberg_pval'] = BH_pval\n",
        "    input_df['-10*log10(BH)'] = np.log10(input_df['Benjamini_Hochberg_pval'].astype(float))*-10\n",
        "    return(input_df)\n",
        "\n",
        "  # Use the performBH_correction() to calcualte PV correction\n",
        "  E2vE3_DF = performBH_correction(E2vE3_DF)\n",
        "  E4vE3_DF = performBH_correction(E4vE3_DF)\n",
        "  E4vE2_DF = performBH_correction(E4vE2_DF)\n",
        "\n",
        "  # Store the dataframes with the PV and FC calculations\n",
        "  E2vE3_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E2vE3_PVFC_{fileName}.csv',index=True)\n",
        "  E4vE3_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E4vE3_PVFC_{fileName}.csv',index=True)\n",
        "  E4vE2_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E4vE2_PVFC_{fileName}.csv',index=True)\n",
        "\n",
        "  # Create Volcano plots for the comparisons\n",
        "  def plotVP(input_df,comparison=None):\n",
        "    df = input_df\n",
        "    sns.set_style('white')\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.set(style='white', context='talk')\n",
        "    plt.axvline(x=FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axvline(x=-FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axhline(y=significance_cutoff, xmin=0, xmax=1, linestyle=':',color='gray')\n",
        "    plt.xlabel(\"Fold_change\")\n",
        "    plt.ylabel(\"-10*log10(BH)\")\n",
        "    plt.title(f'P-value vs. Fold Change',fontsize=30)\n",
        "    plt.xlim(-6, 6)\n",
        "\n",
        "    df['cutoff'] = ''\n",
        "\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] < significance_cutoff),['cutoff']] = 100\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 100\n",
        "    df.loc[((df['Fold_change'] >= FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 250\n",
        "    df.loc[((df['Fold_change'] <= -FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 15\n",
        "\n",
        "    VP = sns.scatterplot(x='Fold_change',\n",
        "                        y='-10*log10(BH)',\n",
        "                        data=df,\n",
        "                        legend=False,\n",
        "                        palette='cividis',\n",
        "                        hue='cutoff',\n",
        "                        s = 350,\n",
        "                        alpha=0.8)\n",
        "\n",
        "    plt.savefig(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Figures/{fileName}_{comparison}_VP.svg',format=\"svg\",transparent=True)\n",
        "\n",
        "    return(VP)\n",
        "\n",
        "  plotVP(E2vE3_DF,'E2vE3')\n",
        "  plotVP(E4vE3_DF,'E4vE3')\n",
        "  plotVP(E4vE2_DF,'E4vE2')\n",
        "\n",
        "  continue"
      ],
      "metadata": {
        "id": "qC2YEH9NT0oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following section combines the fold changes and P-values from each dataset into a single P-value and Fold Change each protein."
      ],
      "metadata": {
        "id": "EUt6MI6PVofE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "where_are_the_files = f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC'\n",
        "os.chdir(where_are_the_files)\n",
        "all_file_names = [i for i in glob.glob('*.{}'.format('csv'))]\n",
        "\n",
        "primary_E2vE3_PVFC = pd.DataFrame()\n",
        "primary_E4vE3_PVFC = pd.DataFrame()\n",
        "primary_E4vE2_PVFC = pd.DataFrame()\n",
        "\n",
        "for dataSet in all_file_names:\n",
        "  fileName = dataSet.strip('.csv')\n",
        "  PVFC_DF = pd.read_csv(dataSet).set_index('Accession').filter(regex='P_val|Fold_change')\n",
        "  PVFC_DF['Fold_change'] = PVFC_DF['Fold_change']\n",
        "\n",
        "  if 'E2vE3' in dataSet:\n",
        "    primary_E2vE3_PVFC = pd.concat([primary_E2vE3_PVFC,PVFC_DF])\n",
        "  elif 'E4vE3' in dataSet:\n",
        "    primary_E4vE3_PVFC = pd.concat([primary_E4vE3_PVFC,PVFC_DF])\n",
        "  elif 'E4vE2' in dataSet:\n",
        "    primary_E4vE2_PVFC = pd.concat([primary_E4vE2_PVFC,PVFC_DF])\n",
        "  continue\n",
        "\n",
        "def combine_PV_and_FC(input_PVFC_DF):\n",
        "  protein_list = input_PVFC_DF.index.drop_duplicates()\n",
        "\n",
        "  pv_df = input_PVFC_DF.copy().filter(regex='P_val')\n",
        "  PV_dict = {}\n",
        "  combined_PV_dict = {}\n",
        "\n",
        "  for protein in protein_list:\n",
        "    vals = pv_df['P_val'][protein].tolist()\n",
        "    pv_array = np.array(vals, ndmin=1)\n",
        "    combined_pv = combine_pvalues(pv_array,method='mudholkar_george',weights=None)[1]\n",
        "    PV_dict[protein] = vals\n",
        "    combined_PV_dict[protein] = combined_pv\n",
        "\n",
        "# Calculate Average fold-change\n",
        "  fc_df = input_PVFC_DF.copy().filter(regex='Fold_change')\n",
        "  fc_df = 2**(fc_df)\n",
        "  fc_df = fc_df.reset_index()\n",
        "  fc_df = fc_df.groupby(by='Accession',as_index=True).mean()\n",
        "  fc_df = np.log2(fc_df['Fold_change'])\n",
        "\n",
        "  clean_PV_DF = pd.DataFrame.from_dict(combined_PV_dict.items()).rename(columns={0 : 'Accession',1 : 'fisher_combined_PV'}).set_index('Accession')\n",
        "  cleaned_PVFC_DF = pd.concat([clean_PV_DF,fc_df],axis=1)\n",
        "  cleaned_PVFC_DF['-10log10(comb_PV)'] = np.log10(cleaned_PVFC_DF['fisher_combined_PV'])*-10\n",
        "\n",
        "  def performBH_correction(input_df):\n",
        "    input_df['Benjamini_Hochberg_pval'] = None\n",
        "    input_df = input_df.reset_index()\n",
        "    #sort cleaned_df by pvalue jc mod\n",
        "    input_df = input_df.sort_values(by='fisher_combined_PV')\n",
        "    input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "    #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "    total_rows = len(input_df.index)\n",
        "    for row in input_df.itertuples():\n",
        "      BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "      input_df.at[row.Index, 'Benjamini_Hochberg_pval'] = BH_pval\n",
        "    input_df['-10*log10(BH)'] = np.log10(input_df['Benjamini_Hochberg_pval'].astype(float))*-10\n",
        "    return(input_df)\n",
        "\n",
        "  cleaned_PVFC_DF = performBH_correction(cleaned_PVFC_DF).set_index('Accession')\n",
        "\n",
        "  def plotVP(input_df):\n",
        "    df = input_df\n",
        "    sns.set_style('white')\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.set(style='white', context='talk')\n",
        "    plt.axvline(x=FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axvline(x=-FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axhline(y=significance_cutoff, xmin=0, xmax=1, linestyle=':',color='gray')\n",
        "    plt.xlabel(\"Fold_change\")\n",
        "    plt.ylabel(\"-10*log10(BH)\")\n",
        "    plt.title(f'P-value vs. Fold Change',fontsize=30)\n",
        "    # plt.xlim(-4,4)\n",
        "\n",
        "    df['cutoff'] = ''\n",
        "\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] < significance_cutoff),['cutoff']] = '-'\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] > significance_cutoff),['cutoff']] = '-'\n",
        "    df.loc[((df['Fold_change'] >= FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 'sigUp'\n",
        "    df.loc[((df['Fold_change'] <= -FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 'sigDown'\n",
        "\n",
        "    sigUp = len(df.loc[(df['cutoff'] == 'sigUp'),['cutoff']].index)\n",
        "    sigDown = len(df.loc[(df['cutoff'] == 'sigDown'),['cutoff']].index)\n",
        "\n",
        "    print(sigUp,sigDown)\n",
        "\n",
        "    VP = sns.scatterplot(x='Fold_change',\n",
        "                        y='-10*log10(BH)',\n",
        "                        data=df,\n",
        "                        legend=False,\n",
        "                        palette='cividis',\n",
        "                        hue='cutoff',\n",
        "                        s = 350,\n",
        "                        alpha=0.8)\n",
        "    return(VP)\n",
        "\n",
        "  plotVP(cleaned_PVFC_DF)\n",
        "  return(cleaned_PVFC_DF)\n",
        "\n",
        "primary_E2vE3_stats = combine_PV_and_FC(primary_E2vE3_PVFC)\n",
        "primary_E4vE3_stats = combine_PV_and_FC(primary_E4vE3_PVFC)\n",
        "primary_E4vE2_stats = combine_PV_and_FC(primary_E4vE2_PVFC)\n",
        "\n",
        "primary_E2vE3_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E2vE3_primary_PVFC.csv',index=True)\n",
        "primary_E4vE3_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE3_primary_PVFC.csv',index=True)\n",
        "primary_E4vE2_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE2_primary_PVFC.csv',index=True)\n"
      ],
      "metadata": {
        "id": "QymmnT1sVRdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This portion requires a StringDB Multiprotein Tool output, an ID map from uniprot (Accession to Gene Name), and abundance fold changes from the analysis."
      ],
      "metadata": {
        "id": "EQHtnuMoYb2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def string_conc_analysis(comparison = None):\n",
        "  # # Prepare the dataframe with the fold changes for each protein in each comparison\n",
        "  comparison_DF = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{FC_sex}/primaryy_PVFC/{comparison}_primary_PVFC.csv').set_index('Accession').rename(columns={'Fold_change':f'{comparison}_Fold_change'}).filter(regex=f'{comparison}|BH')\n",
        "\n",
        "  # Scale Fold Change Data using Range Scaling\n",
        "  def range_scaling(df):\n",
        "    df_min = np.min(df[f'{comparison}_Fold_change'])\n",
        "    df_max = np.max(df[f'{comparison}_Fold_change'])\n",
        "    df_mean = np.mean(df[f'{comparison}_Fold_change'])\n",
        "    df_std = np.std(df[f'{comparison}_Fold_change'])\n",
        "\n",
        "    df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] - df_mean\n",
        "    # df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] / np.sqrt(df_std)\n",
        "    # df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] * (df_mean/df_std)\n",
        "    df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] / (df_max - df_min)\n",
        "\n",
        "    df = df.filter(regex=f'{comparison}|BH')\n",
        "\n",
        "    return(df)\n",
        "\n",
        "  comparison_DF = range_scaling(comparison_DF)\n",
        "\n",
        "  # Reverser Log2 the data for calculations\n",
        "  comparison_DF[f'{comparison}_Fold_change'] = 2**comparison_DF[f'{comparison}_Fold_change']\n",
        "\n",
        "  # Create a list that contains the protein ID used to match the protein names in the String output\n",
        "  protein_list = comparison_DF.index.to_list()\n",
        "  for position,protein in enumerate(protein_list):\n",
        "    protein = protein.split('|')[0]\n",
        "    protein_list[position] = protein\n",
        "\n",
        "  comparison_DF['Accession'] = protein_list\n",
        "\n",
        "  comparison_DF = comparison_DF.set_index('Accession')\n",
        "\n",
        "  # full_comparison_DF = full_comparison_DF[[f'{comparison}_Fold_change','Protein_ID','Accession']].dropna()\n",
        "  full_comparison_DF = comparison_DF\n",
        "\n",
        "  STRING_ID_MAP = pd.read_csv('/content/drive/MyDrive/ApoE Analysis January-11-2023/String_ID_Map.tsv', sep='\\t').rename(columns={'From':'Accession','To':'Protein_ID'})\n",
        "  STRING_ID_MAP = STRING_ID_MAP.sort_values(by='Accession')\n",
        "  STRING_ID_MAP = STRING_ID_MAP.drop_duplicates('Accession').set_index('Accession')\n",
        "\n",
        "  full_comparison_DF = pd.concat([full_comparison_DF,STRING_ID_MAP],axis=1).dropna()\n",
        "\n",
        "  # Read string DB output CSV\n",
        "  string_annot = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/String All Genotype Analysis.tsv',sep='\\t')\n",
        "  string_annot['matching proteins in your network (labels)'] = string_annot['matching proteins in your network (labels)'][:].str.split(r',')\n",
        "\n",
        "  string_annot['Ontology_coverage (%)'] = string_annot['observed gene count']/string_annot['background gene count']\n",
        "  string_annot = string_annot.loc[string_annot['Ontology_coverage (%)'] >= 0.250,]\n",
        "\n",
        "  ontologies_of_interest = ['GO Process','GO Function','GO Component','KEGG','Reactome','WikiPathways']\n",
        "  string_annot = string_annot.loc[string_annot['#category'].isin(ontologies_of_interest)].reset_index(drop=True)\n",
        "\n",
        "  string_annot[[f'{comparison}']] = None\n",
        "  string_annot[f'measurement_count'] = None\n",
        "  string_annot[f'sig_proteins_in_ont'] = None\n",
        "  string_annot[f'sig_proteins_count'] = None\n",
        "\n",
        "  # Create a dataframe column that contains all protein fold changes from the ontology\n",
        "  counter = 0\n",
        "  while counter < len(string_annot.index):\n",
        "    protein_list = string_annot.loc[counter,'matching proteins in your network (labels)']\n",
        "    protein_in_ontology = full_comparison_DF.loc[full_comparison_DF['Protein_ID'].isin(protein_list)]\n",
        "    string_annot[f'{comparison}'][counter] = (protein_in_ontology[f'{comparison}_Fold_change'].values.tolist())\n",
        "    string_annot.loc[counter,f'measurement_count'] = len((protein_in_ontology[f'{comparison}_Fold_change'].values.tolist()))\n",
        "    counter += 1\n",
        "\n",
        "  # Create a column with a list of proteins that have significant change in concentration\n",
        "  sig_proteins = full_comparison_DF.loc[full_comparison_DF['-10*log10(BH)'] > np.log10(0.05)*-10, ]\n",
        "\n",
        "  counter = 0\n",
        "  while counter < len(string_annot.index):\n",
        "\n",
        "    protein_list = string_annot.loc[counter,'matching proteins in your network (labels)']\n",
        "    sig_protein_in_ontology = sig_proteins.loc[sig_proteins['Protein_ID'].isin(protein_list)].index.to_list()\n",
        "    string_annot[f'sig_proteins_in_ont'][counter] = sig_protein_in_ontology\n",
        "    string_annot.loc[counter,f'sig_proteins_count'] = len(sig_protein_in_ontology)\n",
        "    counter += 1\n",
        "\n",
        "  string_annot[f'sig_proteins_ont_coverage'] = string_annot['sig_proteins_count']/string_annot['background gene count']\n",
        "\n",
        "  # Create a column for average ontology fold change and p-value calculated from all protein fold changes.\n",
        "  string_annot[f'{comparison}_PV_from_Ontology_FC'] = None\n",
        "  string_annot[f'{comparison}_Avg_Ontology_FC'] = None\n",
        "\n",
        "  # Use a one-sample t-test for fold changes in the ontology (log2(FC) != 1) and calculate FC average for the ontology\n",
        "  counter = 0\n",
        "\n",
        "  while counter < len(string_annot.index):\n",
        "    if len(string_annot[f'{comparison}'][counter]) > 1:\n",
        "      string_annot.loc[counter,f'{comparison}_Avg_Ontology_FC'] = np.mean(string_annot[f'{comparison}'][counter])\n",
        "      string_annot.loc[counter,f'{comparison}_PV_from_Ontology_FC'] = st.ttest_1samp((string_annot[f'{comparison}'][counter]), 1 , nan_policy='omit', alternative='two-sided')[1]\n",
        "      counter += 1\n",
        "    else:\n",
        "      counter += 1\n",
        "\n",
        "  # Log transform FCs and PVs\n",
        "  string_annot[f'{comparison}_log_PV'] = np.log10(string_annot[f'{comparison}_PV_from_Ontology_FC'].astype(float))*-10\n",
        "  string_annot[f'{comparison}_log_FC'] = np.log2(string_annot[f'{comparison}_Avg_Ontology_FC'].astype(float))*10\n",
        "\n",
        "  string_annot = string_annot.dropna(subset=[f'{comparison}_log_PV'])\n",
        "  string_annot = string_annot.sort_values(by=f'{comparison}_PV_from_Ontology_FC')\n",
        "  string_annot = string_annot.drop_duplicates(subset=['term ID'], keep='first')\n",
        "\n",
        "  # Calculate BH P-value correction\n",
        "  def performBH_correction(input_df,comparison=None):\n",
        "    input_df[f'{comparison}_BH_from_Ontology_FC'] = None\n",
        "    input_df = input_df.loc[input_df[f'{comparison}_log_PV'] > 0,]\n",
        "    input_df = input_df.reset_index()\n",
        "    #sort cleaned_df by pvalue jc mod\n",
        "    input_df = input_df.sort_values(by=f'{comparison}_PV_from_Ontology_FC')\n",
        "    input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "    #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "    total_rows = len(input_df.index)\n",
        "    for row in input_df.itertuples():\n",
        "      BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "      input_df.at[row.Index, f'{comparison}_BH_from_Ontology_FC'] = BH_pval\n",
        "    input_df[f'{comparison}_-10*log10(BH)'] = np.log10(input_df[f'{comparison}_BH_from_Ontology_FC'].astype(float))*-10\n",
        "    return(input_df)\n",
        "\n",
        "  Ontology_Comparison = performBH_correction(string_annot,comparison=f'{comparison}')\n",
        "\n",
        "  Ontology_Comparison.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/Male-Female/String_Ontology_Stats/{comparison}_SDB_Concentration_Stats.csv')\n",
        "\n",
        "  # display(Ontology_Comparison)\n",
        "\n",
        "  return(Ontology_Comparison)\n",
        "\n",
        "string_E2vE3_conc_df = string_conc_analysis(comparison='E2vE3')\n",
        "string_E4vE3_conc_df = string_conc_analysis(comparison='E4vE3')\n",
        "string_E4vE2_conc_df = string_conc_analysis(comparison='E4vE2')"
      ],
      "metadata": {
        "id": "i-sYGXlpZAfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section combines the Rate values output from deuterate and performs an ontology analysis similar to the abundance analysis."
      ],
      "metadata": {
        "id": "EUUo8sBIZgcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_Rate_DF(comparison=None):\n",
        "\n",
        "  protein_ID_guide = pd.read_csv('/content/drive/MyDrive/ApoE Turnover Data/Protein_ID_Guide/Protein_ID_Guide_File.csv').set_index('Accession')\n",
        "\n",
        "  # Create dataframes for rates in each isoform\n",
        "  a2_df = pd.DataFrame()\n",
        "  a3_df = pd.DataFrame()\n",
        "  a4_df = pd.DataFrame()\n",
        "\n",
        "  # Where are the Rate files located\n",
        "  where_are_the_files = f'/content/drive/MyDrive/ApoE Turnover Data/Female_Rates_Peptide_FDR_2'\n",
        "  os.chdir(where_are_the_files)\n",
        "  all_file_names = [i for i in glob.glob('*.{}'.format('csv'))]\n",
        "\n",
        "  full_bkg_df = pd.DataFrame()\n",
        "\n",
        "  # Filter each rate file and create allele specific dataframes\n",
        "  for dataSet in all_file_names:\n",
        "    fileName = dataSet.strip('.csv')\n",
        "    TR_df = pd.read_csv(dataSet).set_index('analyte_id').filter(regex='Combined')\n",
        "\n",
        "    TR_df = TR_df.fillna(0)\n",
        "    TR_df = TR_df.loc[TR_df['Combined uniques'].astype(int) > 1,]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'] != 'Insufficient Timepoints',]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'] != 'value could not be determined',]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'].astype(float) > 0,]\n",
        "    TR_df = TR_df.loc[TR_df['Combined R2'].astype(float) > 0.6,]\n",
        "    TR_df = TR_df.filter(regex='rate')\n",
        "\n",
        "    allele = re.findall('A\\d|M\\d', fileName)[0]\n",
        "    TR_df = TR_df.rename(columns={'Combined rate' : f'{allele}'})\n",
        "    if allele == 'A2':\n",
        "      a2_df = pd.concat([a2_df,TR_df])\n",
        "    elif allele == 'A3':\n",
        "      a3_df = pd.concat([a3_df,TR_df])\n",
        "    elif allele == 'A4':\n",
        "      a4_df = pd.concat([a4_df,TR_df])\n",
        "\n",
        "\n",
        "  # Calculate the average rate for each protein from the different rate files\n",
        "  a2_df['A2'] = a2_df['A2'].astype(float)\n",
        "  a2_df = a2_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a2_std = np.std(np.log2(a2_df['A2']))\n",
        "  a2_df['A2'] = np.log2(a2_df['A2'])\n",
        "\n",
        "  a3_df['A3'] = a3_df['A3'].astype(float)\n",
        "  a3_df = a3_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a3_std = np.std(np.log2(a3_df['A3']))\n",
        "  a3_df['A3'] = np.log2(a3_df['A3'])\n",
        "\n",
        "  a4_df['A4'] = a4_df['A4'].astype(float)\n",
        "  a4_df = a4_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a4_std = np.std(np.log2(a4_df['A4']))\n",
        "  a4_df['A4'] = np.log2(a4_df['A4'])\n",
        "\n",
        "  # Create a dataframe with all the rates and\n",
        "  all_rates = pd.concat([a2_df,a3_df,a4_df,protein_ID_guide],axis=1)\n",
        "\n",
        "  # Create comparison specific dataframes\n",
        "  E2vE3_rates_df = pd.concat([a2_df,a3_df,protein_ID_guide],axis=1).dropna()\n",
        "  E4vE3_rates_df = pd.concat([a3_df,a4_df,protein_ID_guide],axis=1).dropna()\n",
        "  E4vE2_rates_df = pd.concat([a2_df,a4_df,protein_ID_guide],axis=1).dropna()\n",
        "\n",
        "  # Calculate fold change\n",
        "  E2vE3_rates_df['E2vE3_rate_FC'] = ((E2vE3_rates_df['A2'] - E2vE3_rates_df['A3']) )\n",
        "  E4vE3_rates_df['E4vE3_rate_FC'] = ((E4vE3_rates_df['A4'] - E4vE3_rates_df['A3']) )\n",
        "  E4vE2_rates_df['E4vE2_rate_FC'] = ((E4vE2_rates_df['A4'] - E4vE2_rates_df['A2']) )\n",
        "\n",
        "\n",
        "  # Concatenate the the comparison dataframes\n",
        "  Rate_FC_DF = pd.concat([E2vE3_rates_df['E2vE3_rate_FC'],E4vE3_rates_df['E4vE3_rate_FC'],E4vE2_rates_df['E4vE2_rate_FC'],protein_ID_guide],axis=1)\n",
        "  Rate_FC_DF.to_csv('/content/drive/MyDrive/ApoE Turnover Data/Rate_FC_DF')\n",
        "  full_Rate_FC_df = Rate_FC_DF.filter(regex='FC')\n",
        "\n",
        "  print('This function outputs a rate FC dataframe; the FC was calculated with the log2 rate values, FC = [log2(B) - log(A)]')\n",
        "\n",
        "  return(full_Rate_FC_df)\n",
        "\n",
        "def string_rate_analysis(comparison=None):\n",
        "  #Filter and Scale rate data\n",
        "  full_Rate_FC_df = create_Rate_DF(comparison=None)\n",
        "  Rate_FC_df = full_Rate_FC_df.copy()\n",
        "  Rate_FC_df = Rate_FC_df.filter(regex=f'{comparison}|ID').dropna()\n",
        "\n",
        "  def auto_scaling(df):\n",
        "    Rate_FC_DF_mean = np.mean(df[f'{comparison}_rate_FC'])\n",
        "    Rate_FC_DF_std = np.std(df[f'{comparison}_rate_FC'])\n",
        "    df[f'{comparison}_rate_FC'] = df[f'{comparison}_rate_FC'] - Rate_FC_DF_mean\n",
        "    df[f'{comparison}_rate_FC'] = df[f'{comparison}_rate_FC'] / (Rate_FC_DF_std)\n",
        "    # df[f'{comparison}_rate_FC'] = df[f'{comparison}_rate_FC'] * (Rate_FC_DF_mean/Rate_FC_DF_std)\n",
        "    return(df)\n",
        "\n",
        "  scaled_RDF = auto_scaling(Rate_FC_df).filter(regex='rate')\n",
        "\n",
        "  #Match protein accession to protein ID for string analysis\n",
        "  STRING_ID_MAP = pd.read_csv('/content/drive/MyDrive/ApoE Analysis January-11-2023/String_ID_Map.tsv', sep='\\t').rename(columns={'From':'Accession','To':'Protein_ID'})\n",
        "  STRING_ID_MAP = STRING_ID_MAP.sort_values(by='Accession')\n",
        "  STRING_ID_MAP = STRING_ID_MAP.drop_duplicates('Accession').set_index('Accession')\n",
        "\n",
        "  scaled_RDF =scaled_RDF.copy()\n",
        "  scaled_RDF = pd.concat([scaled_RDF,STRING_ID_MAP],axis=1).dropna()\n",
        "  full_Rate_FC_df = scaled_RDF.copy()\n",
        "\n",
        "  # Format the string file to create readable proteins lists for each ontology\n",
        "  string_annot = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/String All Genotype Analysis.tsv',sep='\\t')\n",
        "  string_annot['matching proteins in your network (labels)'] = string_annot['matching proteins in your network (labels)'][:].str.split(r',')\n",
        "\n",
        "  string_annot['Ontology_coverage (%)'] = string_annot['observed gene count']/string_annot['background gene count']\n",
        "  string_annot = string_annot.loc[string_annot['Ontology_coverage (%)'] >= 0.25,]\n",
        "\n",
        "  ontologies_of_interest = ['GO Process','GO Function','GO Component','KEGG','Reactome','WikiPathways']\n",
        "  string_annot = string_annot.loc[string_annot['#category'].isin(ontologies_of_interest)].reset_index(drop=True)\n",
        "  string_annot[[f'{comparison}_Rate']] = None\n",
        "\n",
        "  # Create a dataframe column that contains all protein fold changes from the ontology\n",
        "  counter = 0\n",
        "  while counter < len(string_annot.index):\n",
        "    protein_list = string_annot.loc[counter,'matching proteins in your network (labels)']\n",
        "    protein_in_ontology = full_Rate_FC_df.loc[full_Rate_FC_df['Protein_ID'].isin(protein_list)]\n",
        "    string_annot[f'{comparison}_Rate'][counter] = (protein_in_ontology[f'{comparison}_rate_FC'].dropna()).values.tolist()\n",
        "    counter += 1\n",
        "\n",
        "  # Create a column for average ontology fold change and p-value calculated from all protein fold changes.\n",
        "  counter = 0\n",
        "  string_annot[f'{comparison}_PV_from_Ontology_Rate_FC'] = None\n",
        "  string_annot[f'{comparison}_PV_from_Ontology_Rate_FC'] = None\n",
        "\n",
        "  string_annot[f'{comparison}_Avg_Ontology_Rate_FC'] = None\n",
        "  string_annot[f'{comparison}_Avg_Ontology_Rate_FC'] = None\n",
        "\n",
        "  # Use a one-sample t-test for fold changes in the ontology (FC != 1) and calculate FC average for the ontology\n",
        "  counter = 0\n",
        "  while counter < len(string_annot.index):\n",
        "    if len(string_annot[f'{comparison}_Rate'][counter]) > 2:\n",
        "      string_annot.loc[counter,f'{comparison}_PV_from_Ontology_Rate_FC'] = st.ttest_1samp((string_annot[f'{comparison}_Rate'][counter]), 0 , nan_policy='omit', alternative='two-sided')[1]\n",
        "      string_annot.loc[counter,f'{comparison}_Avg_Ontology_Rate_FC'] = np.mean(string_annot[f'{comparison}_Rate'][counter])\n",
        "      counter += 1\n",
        "    else:\n",
        "      counter += 1\n",
        "\n",
        "  rate_string_analysis = string_annot.copy()\n",
        "\n",
        "  rate_string_analysis[f'{comparison}_Rate_log_PV'] = np.log10(rate_string_analysis[f'{comparison}_PV_from_Ontology_Rate_FC'].astype(float))*-10\n",
        "  rate_string_analysis[f'{comparison}_Rate_log_FC'] = (rate_string_analysis[f'{comparison}_Avg_Ontology_Rate_FC'].astype(float))\n",
        "\n",
        "  rate_string_analysis = rate_string_analysis.dropna(subset=[f'{comparison}_Rate_log_PV'])\n",
        "  rate_string_analysis = rate_string_analysis.sort_values(by=f'{comparison}_PV_from_Ontology_Rate_FC')\n",
        "\n",
        "  # Calculate BH P-value correction\n",
        "  def performBH_correction(input_df,comparison=None):\n",
        "    input_df[f'{comparison}_BH_from_Ontology_Rate_FC'] = None\n",
        "    input_df = input_df.loc[input_df[f'{comparison}_Rate_log_PV'] > 0,]\n",
        "    input_df = input_df.reset_index()\n",
        "    input_df = input_df.sort_values(by=f'{comparison}_PV_from_Ontology_Rate_FC')\n",
        "    input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "    #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "    total_rows = len(input_df.index)\n",
        "    for row in input_df.itertuples():\n",
        "      BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "      input_df.at[row.Index, f'{comparison}_BH_from_Ontology_Rate_FC'] = BH_pval\n",
        "    input_df[f'{comparison}_Rate_-10*log10(BH)'] = np.log10(input_df[f'{comparison}_BH_from_Ontology_Rate_FC'].astype(float))*-10\n",
        "    return(input_df)\n",
        "\n",
        "  rate_string_analysis = performBH_correction(rate_string_analysis,comparison=f'{comparison}')\n",
        "\n",
        "  rate_string_analysis.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/Male-Female/String_Ontology_Stats/{comparison}_SDB_Rate_Stats.csv')\n",
        "\n",
        "  return(rate_string_analysis)\n",
        "\n",
        "string_E2vE3_rate_df = string_rate_analysis(comparison='E2vE3')\n",
        "string_E4vE3_rate_df = string_rate_analysis(comparison='E4vE3')\n",
        "string_E4vE2_rate_df = string_rate_analysis(comparison='E4vE2')"
      ],
      "metadata": {
        "id": "3zWpvePhZfbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jN7J5c-Ob0BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this portion to combine the rate ontology analysis and the abundance ontology analysis from a given comparison. This code will also combine the results from both comparisons and graph the proteostasis plot."
      ],
      "metadata": {
        "id": "KSzlQWnAb1kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code concatenates the rate string analysis and the concentration string analysis\n",
        "def create_RNC_DF(comparison=None):\n",
        "  # Read the concentration SBD analysis\n",
        "  SDB_conc_Stats = pd.read_csv(f'{directory}/Male-Female/String_Ontology_Stats/{comparison}_SDB_Concentration_Stats.csv')\n",
        "  SDB_conc_Stats['Ontology_coverage (%)'] = (SDB_conc_Stats['observed gene count']/SDB_conc_Stats['background gene count'])*100\n",
        "  SDB_conc_Stats = SDB_conc_Stats.set_index('term ID').filter(regex='E\\dvE\\d_log_FC|term.description|PV|labels|observed|BH|coverage|sig').rename(columns={f'{comparison}' : f'{comparison}_FCs',f'{comparison}_log_FC' : f'{comparison}_Conc_log_FC'})\n",
        "\n",
        "  # Read the Rate SBD analysis\n",
        "  SDB_rate_Stats = pd.read_csv(f'{directory}/Male-Female/String_Ontology_Stats/{comparison}_SDB_Rate_Stats.csv')\n",
        "  SDB_rate_Stats = SDB_rate_Stats.set_index('term ID').filter(regex=f'{comparison}_Rate_log_FC|category')\n",
        "  SDB_rate_Stats[f'{comparison}_Rate_log_FC'] = SDB_rate_Stats[f'{comparison}_Rate_log_FC']\n",
        "\n",
        "  # Combined the Concentration and Rate SDB analysis\n",
        "  RnC_Df = pd.concat([SDB_conc_Stats,SDB_rate_Stats],axis=1).dropna()\n",
        "  RnC_Df = RnC_Df.rename(columns={'observed gene count':'Ontology Protein Count'})\n",
        "  RnC_Df[\"term description (Count)\"] = (RnC_Df['term description']).astype(str) + \" \" +\"(\" + (RnC_Df['Ontology Protein Count'].astype(int)).astype(str) + \")\"\n",
        "  RnC_Df = RnC_Df.filter(regex='E\\dvE\\d_log_FC|term.description.|labels|observed|10.BH|coverage|log_FC|count|category|sig')\n",
        "\n",
        "  RnC_Df = RnC_Df.rename(columns={f'sig_proteins_in_ont':f'{comparison}_sig_proteins_in_ont'})\n",
        "  RnC_Df = RnC_Df.rename(columns={f'sig_proteins_count':f'{comparison}_sig_proteins_count'})\n",
        "\n",
        "  RnC_Df = RnC_Df[['#category',\n",
        "                  'term description (Count)',\n",
        "                  f'{comparison}_Conc_log_FC',\n",
        "                  f'{comparison}_Rate_log_FC',\n",
        "                  f'{comparison}_-10*log10(BH)',\n",
        "                  'Ontology_coverage (%)',\n",
        "                  'matching proteins in your network (labels)',\n",
        "                  f'{comparison}_sig_proteins_in_ont',\n",
        "                  f'{comparison}_sig_proteins_count']]\n",
        "\n",
        "\n",
        "  return(RnC_Df)\n",
        "\n",
        "RnC_Df_for_lists_E2vE3 = create_RNC_DF(comparison='E2vE3')\n",
        "RnC_Df_for_lists_E4vE3 = create_RNC_DF(comparison='E4vE3')\n",
        "RnC_Df_for_lists_E4vE2 = create_RNC_DF(comparison='E4vE2')\n",
        "\n",
        "\n",
        "# Create a combined dataframe of both comparisons with only significant ontologies.\n",
        "RnC_Df_E2vE3 = RnC_Df_for_lists_E2vE3.copy()\n",
        "RnC_Df_E4vE3 = RnC_Df_for_lists_E4vE3.copy()\n",
        "\n",
        "RNC_DF_both_comparisons = pd.concat([RnC_Df_E2vE3,RnC_Df_E4vE3],axis=1)\n",
        "RNC_DF_both_comparisons = RNC_DF_both_comparisons#.loc[((RNC_DF_both_comparisons[f'E2vE3_-10*log10(BH)'] > np.log10(0.025)*-10) | (RNC_DF_both_comparisons[f'E4vE3_-10*log10(BH)'] > np.log10(0.025)*-10)), ]\n",
        "\n",
        "RNC_DF_both_comparisons[\"E4_proteostasis\"] = None\n",
        "RNC_DF_both_comparisons[\"E2_proteostasis\"] = None\n",
        "\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E2vE3_Conc_log_FC'] > 0) & (RNC_DF_both_comparisons['E2vE3_Rate_log_FC'] > 0)),'E2_proteostasis'] = 'synthesis'\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E2vE3_Conc_log_FC'] < 0) & (RNC_DF_both_comparisons['E2vE3_Rate_log_FC'] > 0)),'E2_proteostasis'] = 'Degradation'\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E2vE3_Conc_log_FC'] < 0) & (RNC_DF_both_comparisons['E2vE3_Rate_log_FC'] < 0)),'E2_proteostasis'] = 'synthesis'\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E2vE3_Conc_log_FC'] > 0) & (RNC_DF_both_comparisons['E2vE3_Rate_log_FC'] < 0)),'E2_proteostasis'] = 'Degradation'\n",
        "\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E4vE3_Conc_log_FC'] > 0) & (RNC_DF_both_comparisons['E4vE3_Rate_log_FC'] > 0)),'E4_proteostasis'] = 'synthesis'\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E4vE3_Conc_log_FC'] < 0) & (RNC_DF_both_comparisons['E4vE3_Rate_log_FC'] > 0)),'E4_proteostasis'] = 'Degradation'\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E4vE3_Conc_log_FC'] < 0) & (RNC_DF_both_comparisons['E4vE3_Rate_log_FC'] < 0)),'E4_proteostasis'] = 'synthesis'\n",
        "RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E4vE3_Conc_log_FC'] > 0) & (RNC_DF_both_comparisons['E4vE3_Rate_log_FC'] < 0)),'E4_proteostasis'] = 'Degradation'\n",
        "\n",
        "RNC_DF_both_comparisons = RNC_DF_both_comparisons.loc[:,~RNC_DF_both_comparisons.columns.duplicated()].dropna()\n",
        "\n",
        "RNC_DF_both_comparisons = RNC_DF_both_comparisons[['#category','Ontology_coverage (%)','term description (Count)','E2_proteostasis','E4_proteostasis','E2vE3_-10*log10(BH)','E4vE3_-10*log10(BH)','E2vE3_Conc_log_FC','E2vE3_Rate_log_FC','E4vE3_Conc_log_FC','E4vE3_Rate_log_FC','matching proteins in your network (labels)','E2vE3_sig_proteins_in_ont','E2vE3_sig_proteins_count','E4vE3_sig_proteins_in_ont','E4vE3_sig_proteins_count']]\n",
        "\n",
        "sig_RNC_DF_both_comparisons = RNC_DF_both_comparisons.loc[((RNC_DF_both_comparisons['E2vE3_-10*log10(BH)'] > np.log10(0.05)*-10) | (RNC_DF_both_comparisons['E4vE3_-10*log10(BH)'] > np.log10(0.05)*-10)),]\n",
        "sig_RNC_DF_both_comparisons['Ontology_coverage (%)'] = sig_RNC_DF_both_comparisons['Ontology_coverage (%)']\n",
        "\n",
        "\n",
        "\n",
        "colors = ['#00375F','#730707']\n",
        "custom_pal = sns.color_palette(colors)\n",
        "\n",
        "def plot_Quadrants(input_df,\n",
        "           plot_title=None,\n",
        "           Conc_column_name=None,\n",
        "           Rate_column_name=None,\n",
        "           fractions=None,\n",
        "           comparison=None,\n",
        "           style=None,\n",
        "           list_of_interest=None,\n",
        "           Ontology_name=None):\n",
        "\n",
        "  df = input_df.copy().filter(regex='log.FC|term|labels|observed|BH')\n",
        "\n",
        "  plt.figure(figsize=(3,3))\n",
        "\n",
        "  custom_params = {'axes.linewidth': 1}\n",
        "\n",
        "  sns.set(style='ticks', context='paper', color_codes=True,rc=custom_params)\n",
        "\n",
        "  plt.axvline(x=0, ymin=0, ymax=1, linestyle=':',color='gray',linewidth=2)\n",
        "  plt.axhline(y=0, xmin=0, xmax=1, linestyle=':',color='gray',linewidth=2)\n",
        "  plt.xlabel(\"Abundance (arb. unit)\", fontweight ='bold', size=10)\n",
        "  plt.ylabel(\"Rate (arb. unit)\", fontweight ='bold',size=10)\n",
        "  plt.title(f'{comparison} Ontology Abundance vs Rate',fontweight ='bold',size=10, pad=5, wrap=False)\n",
        "  plt.tick_params(axis='both', which='major', labelsize=10,length = 2,color = 'black',width =1)\n",
        "\n",
        "  df[\"significance\"] = \"Insignificant\"\n",
        "  df.loc[(df[f'{comparison}_-10*log10(BH)'] > np.log10(0.025)*-10), 'significance'] = \"Significant\"\n",
        "  df.sort_values(by=f'{comparison}_-10*log10(BH)',inplace=True)\n",
        "\n",
        "  sig_len = len(df.loc[(df[\"significance\"] == 'Significant'),])\n",
        "  insig_len = len(df.loc[(df[\"significance\"] == 'Insignificant'),])\n",
        "\n",
        "  df[\"significance\"] = f\"Insignificant ({insig_len})\"\n",
        "  df.loc[(df[f'{comparison}_-10*log10(BH)'] > np.log10(0.025)*-10), 'significance'] = f\"Significant ({sig_len})\"\n",
        "  df.sort_values(by=f'{comparison}_-10*log10(BH)',inplace=True)\n",
        "\n",
        "\n",
        "  VP = sns.scatterplot(x=f'{comparison}_Conc_log_FC',\n",
        "                      y=f'{comparison}_Rate_log_FC',\n",
        "                      data=df,\n",
        "                      legend=True,\n",
        "                      palette=custom_pal,\n",
        "                      # c='#00375F',\n",
        "                      hue='significance',\n",
        "                      s = 50,\n",
        "                      alpha=0.7,\n",
        "                      edgecolor=\"black\",\n",
        "                      linewidth=0.4,\n",
        "                      hue_order = [f\"Insignificant ({insig_len})\",f\"Significant ({sig_len})\"],\n",
        "                      style=style)\n",
        "\n",
        "  plt.legend(loc=8, prop={'size':8},title_fontsize=10,markerscale=.75,borderpad=0.1,borderaxespad=-5, ncols=2)\n",
        "  sns.despine(top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
        "\n",
        "  plt.savefig(f'{directory}/{FC_sex}/String_Ontology_Stats/{comparison}_Proteostasis_Plot.svg',format=\"svg\",transparent=True, dpi=1200,bbox_inches='tight')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return(VP)\n",
        "\n",
        "\n",
        "plot_Quadrants(RnC_Df_for_lists_E2vE3,\n",
        "              plot_title='Rate vs Abundance',\n",
        "              Conc_column_name=f'E2vE3_log_FC',\n",
        "              Rate_column_name=f'E2vE3_Rate_log_FC',\n",
        "              fractions=f'E2vE3',\n",
        "              comparison=f'E2vE3',\n",
        "              style=None,\n",
        "              list_of_interest=None,\n",
        "              Ontology_name=None)\n",
        "\n",
        "plot_Quadrants(RnC_Df_for_lists_E4vE3,\n",
        "              plot_title='Rate vs Abundance',\n",
        "              Conc_column_name=f'E4vE3_log_FC',\n",
        "              Rate_column_name=f'E4vE3_Rate_log_FC',\n",
        "              fractions=f'E4vE3',\n",
        "              comparison=f'E4vE3',\n",
        "              style=None,\n",
        "              list_of_interest=None,\n",
        "              Ontology_name=None)"
      ],
      "metadata": {
        "id": "7rSB0Bciao2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section uses sections of previous codes to create the boxplots of the ontologies of interest."
      ],
      "metadata": {
        "id": "Ep83bxwPc6si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_comparison_PVFC(comparison=None):\n",
        "  # # Prepare the dataframe with the fold changes for each protein in each comparison\n",
        "  comparison_DF = pd.read_csv(f'{directory}/{FC_sex}/primary_PVFC/{comparison}_primary_PVFC.csv').set_index('Accession').rename(columns={'Fold_change':f'{comparison}_Fold_change'}).filter(regex=f'{comparison}|BH')\n",
        "\n",
        "  # Scale Fold Change Data using Range Scaling\n",
        "  comparison_DF[f'{comparison}_Fold_change'] = comparison_DF[f'{comparison}_Fold_change']\n",
        "\n",
        "  comparison_DF_min = np.min(comparison_DF[f'{comparison}_Fold_change'])\n",
        "  comparison_DF_max = np.max(comparison_DF[f'{comparison}_Fold_change'])\n",
        "  comparison_DF_mean = np.mean(comparison_DF[f'{comparison}_Fold_change'])\n",
        "  comparison_DF[f'{comparison}_Fold_change'] = comparison_DF[f'{comparison}_Fold_change'] - comparison_DF_mean\n",
        "  comparison_DF[f'{comparison}_Fold_change'] = comparison_DF[f'{comparison}_Fold_change'] / (comparison_DF_max - comparison_DF_min)\n",
        "\n",
        "  # Reverser Log2 the data for calculations\n",
        "  comparison_DF[f'{comparison}_Fold_change'] = comparison_DF[f'{comparison}_Fold_change']\n",
        "\n",
        "  # Create a list that contains the protein ID used to match the protein names in the String output\n",
        "  protein_list = comparison_DF.index.to_list()\n",
        "  for position,protein in enumerate(protein_list):\n",
        "    protein = protein.split('|')[0]\n",
        "    protein_list[position] = protein\n",
        "\n",
        "  comparison_DF['Accession'] = protein_list\n",
        "  comparison_DF = comparison_DF.set_index('Accession')\n",
        "\n",
        "  STRING_ID_MAP = pd.read_csv(f'{directory}/String_ID_Map.tsv', sep='\\t').rename(columns={'From':'Accession','To':'Protein_ID'})\n",
        "  STRING_ID_MAP = STRING_ID_MAP.sort_values(by='Accession')\n",
        "  STRING_ID_MAP = STRING_ID_MAP.drop_duplicates('Accession').set_index('Accession')\n",
        "\n",
        "  comparison_DF = pd.concat([comparison_DF,STRING_ID_MAP],axis=1).dropna()\n",
        "\n",
        "  return(comparison_DF)\n",
        "\n",
        "def create_Rate_DF(comparison=None):\n",
        "  # Create dataframes for rates in each isoform\n",
        "  a2_df = pd.DataFrame()\n",
        "  a3_df = pd.DataFrame()\n",
        "  a4_df = pd.DataFrame()\n",
        "\n",
        "  # Where are the Rate files located\n",
        "  where_are_the_files = f'/content/drive/MyDrive/ApoE Turnover Data/Female_Rates_Peptide_FDR_2'\n",
        "  os.chdir(where_are_the_files)\n",
        "  all_file_names = [i for i in glob.glob('*.{}'.format('csv'))]\n",
        "\n",
        "  full_bkg_df = pd.DataFrame()\n",
        "\n",
        "  # Filter each rate file and create allele specific dataframes\n",
        "  for dataSet in all_file_names:\n",
        "    fileName = dataSet.strip('.csv')\n",
        "    TR_df = pd.read_csv(dataSet).set_index('analyte_id')\n",
        "\n",
        "    TR_df = TR_df.fillna(0)\n",
        "    TR_df = TR_df.loc[TR_df['Combined uniques'].astype(int) > 1,]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'] != 'Insufficient Timepoints',]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'] != 'value could not be determined',]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'].astype(float) > 0,]\n",
        "    TR_df = TR_df.loc[TR_df['Combined R2'].astype(float) > 0.6,]\n",
        "\n",
        "    TR_a2_df = TR_df.loc[TR_df['group_name'] == 'A2',].copy().filter(regex='Combined rate')\n",
        "    TR_a3_df = TR_df.loc[TR_df['group_name'] == 'A3',].copy().filter(regex='Combined rate')\n",
        "    TR_a4_df = TR_df.loc[TR_df['group_name'] == 'A4',].copy().filter(regex='Combined rate')\n",
        "\n",
        "    TR_a2_df = TR_a2_df.rename(columns={'Combined rate' : f'A2_Combined_rate'})\n",
        "    TR_a3_df = TR_a3_df.rename(columns={'Combined rate' : f'A3_Combined_rate'})\n",
        "    TR_a4_df = TR_a4_df.rename(columns={'Combined rate' : f'A4_Combined_rate'})\n",
        "\n",
        "    a2_df = pd.concat([a2_df,TR_a2_df])\n",
        "    a3_df = pd.concat([a3_df,TR_a3_df])\n",
        "    a4_df = pd.concat([a4_df,TR_a4_df])\n",
        "\n",
        "\n",
        "  # Calculate the average rate for each protein from the different rate files\n",
        "  a2_df['A2_Combined_rate'] = a2_df['A2_Combined_rate'].astype(float)\n",
        "  a2_df = a2_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a2_df['A2_Combined_rate'] = np.log2(a2_df['A2_Combined_rate'])\n",
        "\n",
        "  a3_df['A3_Combined_rate'] = a3_df['A3_Combined_rate'].astype(float)\n",
        "  a3_df = a3_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a3_df['A3_Combined_rate'] = np.log2(a3_df['A3_Combined_rate'])\n",
        "\n",
        "  a4_df['A4_Combined_rate'] = a4_df['A4_Combined_rate'].astype(float)\n",
        "  a4_df = a4_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a4_df['A4_Combined_rate'] = np.log2(a4_df['A4_Combined_rate'])\n",
        "\n",
        "  # Create a dataframe with all the rates and\n",
        "  all_rates = pd.concat([a2_df,a3_df,a4_df],axis=1)\n",
        "\n",
        "  # Create comparison specific dataframes\n",
        "  E2vE3_rates_df = pd.concat([a2_df,a3_df],axis=1).dropna()\n",
        "  E4vE3_rates_df = pd.concat([a3_df,a4_df],axis=1).dropna()\n",
        "  E4vE2_rates_df = pd.concat([a2_df,a4_df],axis=1).dropna()\n",
        "\n",
        "  # Calculate fold change\n",
        "  E2vE3_rates_df['E2vE3_Fold_change'] = ((E2vE3_rates_df['A2_Combined_rate'] - E2vE3_rates_df['A3_Combined_rate']) )\n",
        "  E4vE3_rates_df['E4vE3_Fold_change'] = ((E4vE3_rates_df['A4_Combined_rate'] - E4vE3_rates_df['A3_Combined_rate']) )\n",
        "  E4vE2_rates_df['E4vE2_Fold_change'] = ((E4vE2_rates_df['A4_Combined_rate'] - E4vE2_rates_df['A2_Combined_rate']) )\n",
        "\n",
        "  # Concatenate the the comparison dataframes\n",
        "  Rate_FC_DF = pd.concat([E2vE3_rates_df['E2vE3_Fold_change'],E4vE3_rates_df['E4vE3_Fold_change'],E4vE2_rates_df['E4vE2_Fold_change']],axis=1)\n",
        "\n",
        "  print('This function outputs a rate FC dataframe; the FC was calculated with the log2 rate values, FC = [log2(B) - log(A)]')\n",
        "\n",
        "  comparison_Fold_change_df = Rate_FC_DF.copy().filter(regex=f'{comparison}|ID').dropna()\n",
        "\n",
        "  def auto_scaling(df):\n",
        "    Rate_FC_DF_mean = np.mean(df[f'{comparison}_Fold_change'])\n",
        "    Rate_FC_DF_std = np.std(df[f'{comparison}_Fold_change'])\n",
        "    df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] - Rate_FC_DF_mean\n",
        "    df[f'{comparison}_Fold_change'] = df[f'{comparison}_Fold_change'] / (Rate_FC_DF_std)\n",
        "    return(df)\n",
        "\n",
        "  scaled_RDF = auto_scaling(comparison_Fold_change_df).filter(regex='Fold_change')\n",
        "\n",
        "  STRING_ID_MAP = pd.read_csv('/content/drive/MyDrive/ApoE Analysis January-11-2023/String_ID_Map.tsv', sep='\\t').rename(columns={'From':'Accession','To':'Protein_ID'})\n",
        "  STRING_ID_MAP = STRING_ID_MAP.sort_values(by='Accession')\n",
        "  STRING_ID_MAP = STRING_ID_MAP.drop_duplicates('Accession').set_index('Accession')\n",
        "\n",
        "  scaled_RDF = scaled_RDF.copy()\n",
        "\n",
        "  scaled_RDF = pd.concat([scaled_RDF,STRING_ID_MAP],axis=1).dropna()\n",
        "\n",
        "  return(scaled_RDF)\n",
        "\n",
        "def create_ont_of_int_df(comparison=None):\n",
        "  ontologies_of_interest_df = pd.read_csv('/content/drive/MyDrive/ApoE Analysis January-11-2023/ontologies_for_figures.tsv',sep='\\t')\n",
        "  ontologies_of_interest_df = ontologies_of_interest_df#.loc[((ontologies_of_interest_df[f'E2vE3_-10*log10(BH)'] > np.log10(0.025)*-10) | (ontologies_of_interest_df[f'E4vE3_-10*log10(BH)'] > np.log10(0.025)*-10)), ]\n",
        "\n",
        "  ontologies_of_interest_df[\"E4_proteostasis\"] = None\n",
        "  ontologies_of_interest_df[\"E2_proteostasis\"] = None\n",
        "\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E2vE3_Conc_log_FC'] > 0) & (ontologies_of_interest_df['E2vE3_Rate_log_FC'] > 0)),'E2_proteostasis'] = 'synthesis'\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E2vE3_Conc_log_FC'] < 0) & (ontologies_of_interest_df['E2vE3_Rate_log_FC'] > 0)),'E2_proteostasis'] = 'Degradation'\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E2vE3_Conc_log_FC'] < 0) & (ontologies_of_interest_df['E2vE3_Rate_log_FC'] < 0)),'E2_proteostasis'] = 'synthesis'\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E2vE3_Conc_log_FC'] > 0) & (ontologies_of_interest_df['E2vE3_Rate_log_FC'] < 0)),'E2_proteostasis'] = 'Degradation'\n",
        "\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E4vE3_Conc_log_FC'] > 0) & (ontologies_of_interest_df['E4vE3_Rate_log_FC'] > 0)),'E4_proteostasis'] = 'synthesis'\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E4vE3_Conc_log_FC'] < 0) & (ontologies_of_interest_df['E4vE3_Rate_log_FC'] > 0)),'E4_proteostasis'] = 'Degradation'\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E4vE3_Conc_log_FC'] < 0) & (ontologies_of_interest_df['E4vE3_Rate_log_FC'] < 0)),'E4_proteostasis'] = 'synthesis'\n",
        "  ontologies_of_interest_df.loc[((ontologies_of_interest_df['E4vE3_Conc_log_FC'] > 0) & (ontologies_of_interest_df['E4vE3_Rate_log_FC'] < 0)),'E4_proteostasis'] = 'Degradation'\n",
        "\n",
        "  return(ontologies_of_interest_df)\n",
        "\n",
        "ontologies_of_interest_df = create_ont_of_int_df(comparison=f'{comparison}')\n",
        "\n",
        "\n",
        "def ontology_group_boxp_n_heatm(ontologies_of_interest_df=None,comparison=None):\n",
        "  all_sig_proteins = pd.DataFrame()\n",
        "\n",
        "  comparison_DF = create_comparison_PVFC(comparison=f'{comparison}')\n",
        "  scaled_RDF = create_Rate_DF(comparison=f'{comparison}')\n",
        "  ontologies_of_interest_df = create_ont_of_int_df(comparison=f'{comparison}')\n",
        "\n",
        "  ontology_groups_dict = {}\n",
        "  ontology_groups = ontologies_of_interest_df['ontology_group'].drop_duplicates().to_list()\n",
        "  ontologies_of_interest_df = ontologies_of_interest_df.loc[ontologies_of_interest_df[f'{comparison}_-10*log10(BH)'] > np.log10(0.05)*-10,]\n",
        "\n",
        "  for ontology_group in ontology_groups:\n",
        "    ontologies_in_group = ontologies_of_interest_df.loc[ontologies_of_interest_df['ontology_group'] == ontology_group,'term ID'].values.tolist()\n",
        "    ontology_groups_dict[ontology_group] = ontologies_in_group\n",
        "    continue\n",
        "\n",
        "  all_top_proteins = pd.DataFrame()\n",
        "\n",
        "  for Ontology_name, ontologies in ontology_groups_dict.items():\n",
        "    # Read string DB output CSV\n",
        "    string_annot = pd.read_csv(f'{directory}/String All Genotype Analysis.tsv',sep='\\t')\n",
        "    string_annot['matching proteins in your network (labels)'] = string_annot['matching proteins in your network (labels)'][:].str.split(r',')\n",
        "\n",
        "    # string_annot = string_annot.rename(columns={'observed gene count':'Ontology Protein Count'})\n",
        "    string_annot[\"term description (Count)\"] = (string_annot['term description']).astype(str) + \" \" +\"(\" + (string_annot['observed gene count'].astype(int)).astype(str) + \")\"\n",
        "    string_annot['Ontology_coverage (%)'] = string_annot['observed gene count']/string_annot['background gene count']\n",
        "    string_annot = string_annot.loc[string_annot['Ontology_coverage (%)'] >= 0.25,]\n",
        "\n",
        "    ontologies_of_interest = ['GO Process','GO Function','GO Component','KEGG','Reactome','WikiPathways']\n",
        "    string_annot = string_annot.loc[string_annot['#category'].isin(ontologies_of_interest)].reset_index(drop=True)\n",
        "    string_annot = string_annot.loc[string_annot['term ID'].isin(ontologies)].reset_index(drop=True)\n",
        "    proteins_in_ontology = pd.DataFrame()\n",
        "    term_IDs = string_annot['term ID'].values.tolist()\n",
        "\n",
        "    for term_ID in term_IDs:\n",
        "      protein_list = string_annot.loc[string_annot['term ID'] == term_ID,'matching proteins in your network (labels)'].tolist()[0]\n",
        "      protein_in_ontology = comparison_DF.loc[comparison_DF['Protein_ID'].isin(protein_list),].reset_index(drop=True)\n",
        "      ontology_name = string_annot.loc[string_annot['term ID'] == term_ID,'term description (Count)'].tolist()[0]\n",
        "      protein_in_ontology['Ontology'] = ontology_name\n",
        "      protein_in_ontology['Abundance_FC/Rate_FC'] = 'Abundance'\n",
        "      proteins_in_ontology = pd.concat([proteins_in_ontology,protein_in_ontology],ignore_index=True)\n",
        "\n",
        "    proteins_in_ontology[f'{comparison}_Fold_change'] = proteins_in_ontology[f'{comparison}_Fold_change']*10\n",
        "\n",
        "    proteins_in_ontology = proteins_in_ontology.sort_values(by='Ontology')\n",
        "\n",
        "    for term_ID in term_IDs:\n",
        "      protein_list = string_annot.loc[string_annot['term ID'] == term_ID,'matching proteins in your network (labels)'].tolist()[0]\n",
        "      protein_in_ontology = scaled_RDF.loc[scaled_RDF['Protein_ID'].isin(protein_list),].reset_index(drop=True).dropna()\n",
        "      ontology_name = string_annot.loc[string_annot['term ID'] == term_ID,'term description (Count)'].tolist()[0]\n",
        "      protein_in_ontology['Ontology'] = ontology_name\n",
        "      protein_in_ontology['Abundance_FC/Rate_FC'] = 'Rate'\n",
        "      proteins_in_ontology = pd.concat([proteins_in_ontology,protein_in_ontology],ignore_index=True)\n",
        "\n",
        "    def create_box_plot(input_df=None):\n",
        "      plt.figure(figsize=(1, 1))\n",
        "      plt_size = len(ontologies)*.99\n",
        "      # plt.figure(figsize=(6,plt_size))\n",
        "      plt.figure(figsize=(3.5,plt_size))\n",
        "      proteins_in_ontology = input_df.copy()\n",
        "      proteins_in_ontology['Ontology'] =  ['\\n'.join(wrap(l, 25)) for l in proteins_in_ontology['Ontology']]\n",
        "\n",
        "      flierprops = dict(markerfacecolor='gray', markersize=3,\n",
        "                        linestyle='none', markeredgecolor='gray',alpha=0.5)\n",
        "\n",
        "      # make grouped boxplot and save it in a variable\n",
        "      bp = sns.boxplot(x=f'{comparison}_Fold_change',\n",
        "                       y='Ontology',\n",
        "                      data=proteins_in_ontology,\n",
        "                      palette=\"colorblind\",\n",
        "                      hue='Abundance_FC/Rate_FC',\n",
        "                      fliersize=3,\n",
        "                      flierprops=flierprops,\n",
        "                      saturation=1,\n",
        "                      showmeans=True,\n",
        "                      meanprops={\"marker\": \"o\",\n",
        "                         \"markeredgecolor\": \"black\",\n",
        "                         \"markersize\": \"5\"},\n",
        "                      showfliers = False)\n",
        "\n",
        "      bp = sns.stripplot(x=f'{comparison}_Fold_change',\n",
        "                       y='Ontology',\n",
        "                      data=proteins_in_ontology,\n",
        "                      palette=\"colorblind\",\n",
        "                      hue='Abundance_FC/Rate_FC')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      handles, labels = bp.get_legend_handles_labels()\n",
        "\n",
        "      # specify just one legend\n",
        "      l = plt.legend(handles[0:2], labels[0:2])\n",
        "      plt.xlabel(\"Fold Change (arb. units)\", fontweight ='bold', size=10)\n",
        "      plt.ylabel(\"Protein Ontology (Protein Count)\", fontweight ='bold',size=10)\n",
        "\n",
        "      custom_params = {'axes.linewidth':2}\n",
        "\n",
        "      sns.set(style='ticks', context='paper',font='sans-serif', font_scale=1, color_codes=True,rc=custom_params)\n",
        "\n",
        "      plt.axvline(x=change_cutoff, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "      plt.axvline(x=-change_cutoff, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "      # plt.title(f'{Ontology_name} Ontologies \\n Abundance and Rate\\n{comparison}',fontsize=15,fontweight ='bold')\n",
        "      plt.title(f'{Ontology_name} Ontologies\\n{comparison}',fontsize=12,fontweight ='bold',loc='center')\n",
        "      plt.tick_params(axis='both', which='major', labelsize=10,length = 5,color = 'black',width =3)\n",
        "      plt.yticks(rotation=30)\n",
        "      plt.axvline(x=change_cutoff, ymin=0, ymax=1, linestyle='--',color='red')\n",
        "\n",
        "      bp.legend(prop={'size':10},title_fontsize=10,markerscale=2,borderpad=0.5,loc=8,borderaxespad=-6, ncols=2)\n",
        "\n",
        "      sns.despine(top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
        "\n",
        "      # plt.savefig(f'{directory}/Male-Female/Figures/{Ontology_name}_{comparison}_RnC_Plot.svg',format=\"svg\",transparent=True,bbox_inches='tight')\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "    create_box_plot(input_df=proteins_in_ontology)\n",
        "\n",
        "    continue\n",
        "  return(proteins_in_ontology)\n",
        "\n",
        "\n",
        "ontology_group_boxp_n_heatm(ontologies_of_interest_df=ontologies_of_interest_df,comparison='E4vE3')\n",
        "ontology_group_boxp_n_heatm(ontologies_of_interest_df=ontologies_of_interest_df,comparison='E2vE3')"
      ],
      "metadata": {
        "id": "as3T67YLcStn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
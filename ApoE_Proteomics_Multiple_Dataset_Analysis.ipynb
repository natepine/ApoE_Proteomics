{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1IdYB-yvK3B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install bioinfokit\n",
        "!pip install seaborn\n",
        "!pip install seaborn[stats]\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from pandas.core.algorithms import duplicated\n",
        "from pandas.errors import AccessorRegistrationWarning\n",
        "from scipy import stats as st\n",
        "from scipy.stats import ttest_ind_from_stats\n",
        "from scipy.stats import combine_pvalues\n",
        "from bioinfokit import analys, visuz\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from numpy.ma.core import mean\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from sklearn.impute import KNNImputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter Settings\n",
        "significance_cutoff = np.log10(0.025)*-10\n",
        "FC = np.log2(1.5)\n",
        "sex = 'Male-Female'\n",
        "fraction = 'BCM'"
      ],
      "metadata": {
        "id": "bKpk7kYwixu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NORMALIZATION AND STATS\n",
        "from numpy.core.shape_base import atleast_3d\n",
        "# Parameters\n",
        "where_are_the_files = f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Peaks_DB_Data'\n",
        "os.chdir(where_are_the_files)\n",
        "all_file_names = [i for i in glob.glob('*.{}'.format('csv'))]\n",
        "\n",
        "E2vE3_PVFC_DF = pd.DataFrame()\n",
        "E4vE3_PVFC_DF = pd.DataFrame()\n",
        "E4vE2_PVFC_DF = pd.DataFrame()\n",
        "\n",
        "for dataSet in all_file_names:\n",
        "  fileName = dataSet.strip('.csv')\n",
        "  dataDF = pd.read_csv(dataSet)\n",
        "\n",
        "  # Read CSV, set the 'Accession' as the index, filter out proteins with <2 unique peptides, and filter out non-area data.\n",
        "  dataDF = dataDF.set_index('Accession')\n",
        "  dataDF = dataDF.loc[(dataDF['#Unique'] >= 2),]\n",
        "  dataDF = dataDF.loc[dataDF['Top'] == True,]\n",
        "  dataDF = dataDF.filter(regex='Area')\n",
        "\n",
        "  # Replace 0 with NaN, create allele specific dataframes, count non-zero values, remove proteins with >1 missing value\n",
        "  dataDF = dataDF.replace(np.NaN,0)\n",
        "  dataDF['nonZero_Count'] = (dataDF != 0).astype(int).sum(axis=1)\n",
        "\n",
        "  E2_DF = dataDF.filter(regex='A2').copy()\n",
        "  E3_DF = dataDF.filter(regex='A3').copy()\n",
        "  E4_DF = dataDF.filter(regex='A4').copy()\n",
        "\n",
        "  cols_E2_DF = len(E2_DF.axes[1])\n",
        "  cols_E3_DF = len(E3_DF.axes[1])\n",
        "  cols_E4_DF = len(E4_DF.axes[1])\n",
        "\n",
        "  E2_DF['E2_nonZero_Count'] = (E2_DF != 0).astype(int).sum(axis=1)\n",
        "  E3_DF['E3_nonZero_Count'] = (E3_DF != 0).astype(int).sum(axis=1)\n",
        "  E4_DF['E4_nonZero_Count'] = (E4_DF != 0).astype(int).sum(axis=1)\n",
        "\n",
        "  E2_DF = E2_DF.loc[(E2_DF['E2_nonZero_Count'] >= (cols_E2_DF-1)),]\n",
        "  E3_DF = E3_DF.loc[(E3_DF['E3_nonZero_Count'] >= (cols_E3_DF-1)),]\n",
        "  E4_DF = E4_DF.loc[(E4_DF['E4_nonZero_Count'] >= (cols_E4_DF-1)),]\n",
        "\n",
        "  E2vE3_comparison_DF = pd.concat([E2_DF,E3_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "  E4vE3_comparison_DF = pd.concat([E4_DF,E3_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "  E4vE2_comparison_DF = pd.concat([E4_DF,E2_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "\n",
        "  E4vE3vE2_comparison_DF = pd.concat([E4_DF,E3_DF,E2_DF],axis=1).dropna().filter(regex='Area').copy()\n",
        "\n",
        "  def slope_normalize(input_df):\n",
        "    # Creat allele comparison dataframes, log2 normalize and replace -inf values with NaN.\n",
        "\n",
        "    df_to_normalize = input_df.copy()\n",
        "    df_to_normalize = np.log2(df_to_normalize)\n",
        "    df_to_normalize = df_to_normalize.replace(-np.inf, np.NaN)\n",
        "\n",
        "    # Normalize Data by the Average\n",
        "    for sample in df_to_normalize.columns:\n",
        "      df_to_normalize[sample] = df_to_normalize[sample] - np.mean(df_to_normalize[sample])\n",
        "    df_to_normalize['protein_avg'] = np.mean(df_to_normalize,axis=1)\n",
        "\n",
        "    # Normalize Data by the Slope\n",
        "    for sample in df_to_normalize.columns:\n",
        "      x= df_to_normalize['protein_avg']\n",
        "      y= df_to_normalize[sample]\n",
        "      mask = ~np.isnan(x) & ~np.isnan(y)\n",
        "      slope = st.linregress(x[mask], y[mask])[0]\n",
        "      df_to_normalize[sample] = df_to_normalize[sample]/slope\n",
        "    normalized_DF = df_to_normalize.filter(regex='Area')\n",
        "\n",
        "    # Impute missing values using KNN imputer\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    normalized_DF = pd.DataFrame(imputer.fit_transform(normalized_DF),columns=normalized_DF.columns,index=normalized_DF.index)\n",
        "    x = normalized_DF.plot.density(figsize=(10, 10))\n",
        "\n",
        "    return(normalized_DF)\n",
        "\n",
        "  E4vE3vE2_norm_DF = slope_normalize(E4vE3vE2_comparison_DF)\n",
        "\n",
        "  E2vE3_norm_DF = E4vE3vE2_norm_DF.filter(regex='A2|A3').copy()\n",
        "  E4vE3_norm_DF = E4vE3vE2_norm_DF.filter(regex='A4|A3').copy()\n",
        "  E4vE2_norm_DF = E4vE3vE2_norm_DF.filter(regex='A4|A2').copy()\n",
        "\n",
        "  E2vE3_norm_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Normalized_Peaks_Data/{fileName}_E2vE3_Normalized_DataSet.csv',index=True)\n",
        "  E4vE3_norm_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Normalized_Peaks_Data/{fileName}_E4vE3_Normalized_DataSet.csv',index=True)\n",
        "  E4vE2_norm_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Normalized_Peaks_Data/{fileName}_E4vE2_Normalized_DataSet.csv',index=True)\n",
        "\n",
        "  # Perform T-test and calculate PV\n",
        "  def stats_calc(input_df,comparison=None):\n",
        "    if comparison == 'E2vE3':\n",
        "      allele_a = 'A3'\n",
        "      allele_b = 'A2'\n",
        "    elif comparison == 'E4vE3':\n",
        "      allele_a = 'A3'\n",
        "      allele_b = 'A4'\n",
        "    elif comparison == 'E4vE2':\n",
        "      allele_a = 'A2'\n",
        "      allele_b = 'A4'\n",
        "\n",
        "    comparison_DF_b = input_df.filter(regex=allele_b).copy()\n",
        "    comparison_DF_a = input_df.filter(regex=allele_a).copy()\n",
        "\n",
        "    comparison_p_val_dict = {}\n",
        "    comparison_FC_dict = {}\n",
        "\n",
        "    for protein in input_df.index.to_list():\n",
        "      #ApoEb vs ApoEa P-value and FC\n",
        "      a = comparison_DF_b.loc[protein,]\n",
        "      b = comparison_DF_a.loc[protein,]\n",
        "\n",
        "      f_PV = st.f_oneway(a, b)[1]\n",
        "\n",
        "      if f_PV > 0.05:\n",
        "        comparison_p_val = st.ttest_ind(a,b, equal_var=True, alternative='two-sided',nan_policy='omit')[1]\n",
        "        comparison_p_val_dict[protein] = comparison_p_val\n",
        "\n",
        "      elif f_PV < 0.05:\n",
        "        comparison_p_val = st.ttest_ind(a,b, equal_var=False, alternative='two-sided',nan_policy='omit')[1]\n",
        "        comparison_p_val_dict[protein] = comparison_p_val\n",
        "\n",
        "# Calculate FC\n",
        "      comparison_FC = (np.mean(b)) - (np.mean(a))\n",
        "      comparison_FC = (np.mean(b)) - (np.mean(a))\n",
        "\n",
        "      comparison_FC_dict[protein] = comparison_FC\n",
        "\n",
        "    comparison_df_PV = pd.DataFrame.from_dict(comparison_p_val_dict.items()).rename(columns={0 : 'Accession',1 : f'{comparison}_PV'}).set_index('Accession')\n",
        "    comparison_df_FC = pd.DataFrame.from_dict(comparison_FC_dict.items()).rename(columns={0 : 'Accession',1 : f'{comparison}_FC'}).set_index('Accession')\n",
        "\n",
        "    comparison_df_PV = pd.melt(comparison_df_PV,ignore_index=False).rename(columns={'value' : 'P_val'})\n",
        "    comparison_df_PV = comparison_df_PV.drop(columns='variable')\n",
        "\n",
        "    comparison_df_FC = pd.melt(comparison_df_FC,ignore_index=False).rename(columns={'value' : 'Fold_change'})\n",
        "\n",
        "    comparison_PVFC = pd.concat([comparison_df_PV,comparison_df_FC],axis=1)\n",
        "\n",
        "    comparison_PVFC['-log10(PV)'] = np.log10(comparison_PVFC['P_val'])*-10\n",
        "    return(comparison_PVFC)\n",
        "\n",
        "  E2vE3_DF = stats_calc(E2vE3_norm_DF,comparison='E2vE3')\n",
        "  E4vE3_DF = stats_calc(E4vE3_norm_DF,comparison='E4vE3')\n",
        "  E4vE2_DF = stats_calc(E4vE2_norm_DF,comparison='E4vE2')\n",
        "\n",
        "  E2vE3_PVFC_DF = pd.concat([E2vE3_PVFC_DF,E2vE3_DF])\n",
        "  E4vE3_PVFC_DF = pd.concat([E4vE3_PVFC_DF,E4vE3_DF])\n",
        "  E4vE2_PVFC_DF = pd.concat([E4vE2_PVFC_DF,E4vE2_DF])\n",
        "\n",
        "  # Calculate BH P-value correction for the DataSet\n",
        "  def performBH_correction(input_df):\n",
        "    input_df['Benjamini_Hochberg_pval'] = None\n",
        "    input_df = input_df.reset_index()\n",
        "    #sort cleaned_df by pvalue jc mod\n",
        "    input_df = input_df.sort_values(by='P_val')\n",
        "    input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "    #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "    total_rows = len(input_df.index)\n",
        "    for row in input_df.itertuples():\n",
        "      BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "      input_df.at[row.Index, 'Benjamini_Hochberg_pval'] = BH_pval\n",
        "    input_df['-10*log10(BH)'] = np.log10(input_df['Benjamini_Hochberg_pval'].astype(float))*-10\n",
        "    return(input_df)\n",
        "\n",
        "  E2vE3_DF = performBH_correction(E2vE3_DF)\n",
        "  E4vE3_DF = performBH_correction(E4vE3_DF)\n",
        "  E4vE2_DF = performBH_correction(E4vE2_DF)\n",
        "\n",
        "  E2vE3_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E2vE3_PVFC_{fileName}.csv',index=True)\n",
        "  E4vE3_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E4vE3_PVFC_{fileName}.csv',index=True)\n",
        "  E4vE2_DF.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E4vE2_PVFC_{fileName}.csv',index=True)\n",
        "\n",
        "  # Create Volcano plots for the comparisons\n",
        "  def plotVP(input_df,comparison=None):\n",
        "    df = input_df\n",
        "    sns.set_style('white')\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.set(style='white', context='talk')\n",
        "    plt.axvline(x=FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axvline(x=-FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axhline(y=significance_cutoff, xmin=0, xmax=1, linestyle=':',color='gray')\n",
        "    plt.xlabel(\"Fold_change\")\n",
        "    plt.ylabel(\"-10*log10(BH)\")\n",
        "    plt.title(f'P-value vs. Fold Change',fontsize=30)\n",
        "    plt.xlim(-6, 6)\n",
        "\n",
        "    df['cutoff'] = ''\n",
        "\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] < significance_cutoff),['cutoff']] = 100\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 100\n",
        "    df.loc[((df['Fold_change'] >= FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 250\n",
        "    df.loc[((df['Fold_change'] <= -FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 15\n",
        "\n",
        "    VP = sns.scatterplot(x='Fold_change',\n",
        "                        y='-10*log10(BH)',\n",
        "                        data=df,\n",
        "                        legend=False,\n",
        "                        palette='cividis',\n",
        "                        hue='cutoff',\n",
        "                        s = 350,\n",
        "                        alpha=0.8)\n",
        "\n",
        "    plt.savefig(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Figures/{fileName}_{comparison}_VP.svg',format=\"svg\",transparent=True)\n",
        "\n",
        "    return(VP)\n",
        "\n",
        "  plotVP(E2vE3_DF,'E2vE3')\n",
        "  plotVP(E4vE3_DF,'E4vE3')\n",
        "  plotVP(E4vE2_DF,'E4vE2')\n",
        "\n",
        "  continue"
      ],
      "metadata": {
        "id": "XqoDy-uLDSKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "where_are_the_files = f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC'\n",
        "os.chdir(where_are_the_files)\n",
        "all_file_names = [i for i in glob.glob('*.{}'.format('csv'))]\n",
        "\n",
        "primary_E2vE3_PVFC = pd.DataFrame()\n",
        "primary_E4vE3_PVFC = pd.DataFrame()\n",
        "primary_E4vE2_PVFC = pd.DataFrame()\n",
        "\n",
        "for dataSet in all_file_names:\n",
        "  fileName = dataSet.strip('.csv')\n",
        "  PVFC_DF = pd.read_csv(dataSet).set_index('Accession').filter(regex='P_val|Fold_change')\n",
        "  # stdev_ = PVFC_DF['Fold_change'].std()\n",
        "  # PVFC_DF['Fold_change'] = PVFC_DF['Fold_change'] / 2**stdev_\n",
        "  PVFC_DF['Fold_change'] = PVFC_DF['Fold_change']\n",
        "\n",
        "  # display(PVFC_DF)\n",
        "  if 'E2vE3' in dataSet:\n",
        "    primary_E2vE3_PVFC = pd.concat([primary_E2vE3_PVFC,PVFC_DF])\n",
        "  elif 'E4vE3' in dataSet:\n",
        "    primary_E4vE3_PVFC = pd.concat([primary_E4vE3_PVFC,PVFC_DF])\n",
        "  elif 'E4vE2' in dataSet:\n",
        "    primary_E4vE2_PVFC = pd.concat([primary_E4vE2_PVFC,PVFC_DF])\n",
        "  continue"
      ],
      "metadata": {
        "id": "qT29mqYg_UFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primary_E2vE3 = primary_E2vE3_PVFC.copy()\n",
        "primary_E4vE3 = primary_E4vE3_PVFC.copy()\n",
        "primary_E4vE2 = primary_E4vE2_PVFC.copy()"
      ],
      "metadata": {
        "id": "fQyK3riOLfb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import combine_pvalues\n",
        "\n",
        "def combine_PV_and_FC(input_PVFC_DF):\n",
        "  protein_list = input_PVFC_DF.index.drop_duplicates()\n",
        "\n",
        "  pv_df = input_PVFC_DF.copy().filter(regex='P_val')\n",
        "  PV_dict = {}\n",
        "  combined_PV_dict = {}\n",
        "\n",
        "  for protein in protein_list:\n",
        "    vals = pv_df['P_val'][protein].tolist()\n",
        "    pv_array = np.array(vals, ndmin=1)\n",
        "    combined_pv = combine_pvalues(pv_array,method='mudholkar_george',weights=None)[1]\n",
        "    PV_dict[protein] = vals\n",
        "    combined_PV_dict[protein] = combined_pv\n",
        "\n",
        "# Calculate Average fold-change\n",
        "  fc_df = input_PVFC_DF.copy().filter(regex='Fold_change')\n",
        "\n",
        "  fc_df = 2**(fc_df)\n",
        "  fc_df = fc_df.reset_index()\n",
        "  fc_df = fc_df.groupby(by='Accession',as_index=True).mean()\n",
        "  # fc_df =(fc_df['Fold_change'])\n",
        "  fc_df = np.log2(fc_df['Fold_change'])\n",
        "\n",
        "  clean_PV_DF = pd.DataFrame.from_dict(combined_PV_dict.items()).rename(columns={0 : 'Accession',1 : 'fisher_combined_PV'}).set_index('Accession')\n",
        "  cleaned_PVFC_DF = pd.concat([clean_PV_DF,fc_df],axis=1)\n",
        "  cleaned_PVFC_DF['-10log10(comb_PV)'] = np.log10(cleaned_PVFC_DF['fisher_combined_PV'])*-10\n",
        "\n",
        "  def performBH_correction(input_df):\n",
        "    input_df['Benjamini_Hochberg_pval'] = None\n",
        "    input_df = input_df.reset_index()\n",
        "    #sort cleaned_df by pvalue jc mod\n",
        "    input_df = input_df.sort_values(by='fisher_combined_PV')\n",
        "    input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "    #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "    total_rows = len(input_df.index)\n",
        "    for row in input_df.itertuples():\n",
        "      BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "      input_df.at[row.Index, 'Benjamini_Hochberg_pval'] = BH_pval\n",
        "    input_df['-10*log10(BH)'] = np.log10(input_df['Benjamini_Hochberg_pval'].astype(float))*-10\n",
        "    return(input_df)\n",
        "\n",
        "  cleaned_PVFC_DF = performBH_correction(cleaned_PVFC_DF).set_index('Accession')\n",
        "\n",
        "  def plotVP(input_df):\n",
        "    df = input_df\n",
        "    sns.set_style('white')\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.set(style='white', context='talk')\n",
        "    plt.axvline(x=FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axvline(x=-FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "    plt.axhline(y=significance_cutoff, xmin=0, xmax=1, linestyle=':',color='gray')\n",
        "    plt.xlabel(\"Fold_change\")\n",
        "    plt.ylabel(\"-10*log10(BH)\")\n",
        "    plt.title(f'P-value vs. Fold Change',fontsize=30)\n",
        "    # plt.xlim(-4,4)\n",
        "\n",
        "    df['cutoff'] = ''\n",
        "\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] < significance_cutoff),['cutoff']] = '-'\n",
        "    df.loc[((df['Fold_change'] < FC) | (df['Fold_change'] > -FC)) & (df['-10*log10(BH)'] > significance_cutoff),['cutoff']] = '-'\n",
        "    df.loc[((df['Fold_change'] >= FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 'sigUp'\n",
        "    df.loc[((df['Fold_change'] <= -FC)) & (df['-10*log10(BH)'] >= significance_cutoff),['cutoff']] = 'sigDown'\n",
        "\n",
        "    sigUp = len(df.loc[(df['cutoff'] == 'sigUp'),['cutoff']].index)\n",
        "    sigDown = len(df.loc[(df['cutoff'] == 'sigDown'),['cutoff']].index)\n",
        "\n",
        "    print(sigUp,sigDown)\n",
        "\n",
        "    VP = sns.scatterplot(x='Fold_change',\n",
        "                        y='-10*log10(BH)',\n",
        "                        data=df,\n",
        "                        legend=False,\n",
        "                        palette='cividis',\n",
        "                        hue='cutoff',\n",
        "                        s = 350,\n",
        "                        alpha=0.8)\n",
        "    return(VP)\n",
        "\n",
        "  plotVP(cleaned_PVFC_DF)\n",
        "\n",
        "  # primary_E2vE3_PVFC.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E2vE3_primary_PVFC.csv',index=True)\n",
        "  # primary_E4vE3_PVFC.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/PVFC/E4vE3_primary_PVFC.csv',index=True)\n",
        "\n",
        "  display(cleaned_PVFC_DF)\n",
        "\n",
        "  return(cleaned_PVFC_DF)\n",
        "\n",
        "primary_E2vE3_stats = combine_PV_and_FC(primary_E2vE3_PVFC)\n",
        "primary_E4vE3_stats = combine_PV_and_FC(primary_E4vE3_PVFC)\n",
        "primary_E4vE2_stats = combine_PV_and_FC(primary_E4vE2_PVFC)\n",
        "\n",
        "primary_E2vE3_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E2vE3_primary_PVFC.csv',index=True)\n",
        "primary_E4vE3_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE3_primary_PVFC.csv',index=True)\n",
        "primary_E4vE2_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE2_primary_PVFC.csv',index=True)\n"
      ],
      "metadata": {
        "id": "_vBZt-6SA3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E2vE3_primary_PVFC_df = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E2vE3_primary_PVFC.csv').set_index('Accession').filter(regex='Fold').rename(columns={'Fold_change':'E2vE3_Fold_change'})\n",
        "E4vE3_primary_PVFC_df = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE3_primary_PVFC.csv').set_index('Accession').filter(regex='Fold').rename(columns={'Fold_change':'E4vE3_Fold_change'})\n",
        "E4vE2_primary_PVFC_df = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE2_primary_PVFC.csv').set_index('Accession').filter(regex='Fold').rename(columns={'Fold_change':'E4vE2_Fold_change'})\n",
        "\n",
        "All_genotype_Fold_changes = pd.concat([E2vE3_primary_PVFC_df,E4vE3_primary_PVFC_df,E4vE2_primary_PVFC_df],axis=1)\n",
        "\n",
        "All_genotype_Fold_changes.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/All_Genotype_Fold_Changes/All_genotype_Fold_changes.csv',index=True)\n",
        "\n",
        "def plotVP(input_df):\n",
        "  df = input_df\n",
        "  sns.set_style('white')\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  sns.set(style='white', context='talk')\n",
        "  plt.axvline(x=FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "  plt.axvline(x=-FC, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "  plt.axhline(y=FC, xmin=0, xmax=1, linestyle=':',color='gray')\n",
        "  plt.axhline(y=-FC, xmin=0, xmax=1, linestyle=':',color='gray')\n",
        "  # plt.xlabel(\"Fold_change\")\n",
        "  # plt.ylabel(\"-10*log10(BH)\")\n",
        "  plt.title(f'Fold Change vs. Fold Change',fontsize=30)\n",
        "  # plt.xlim(-3, 3)\n",
        "\n",
        "  VP = sns.scatterplot(x='E2vE3_Fold_change',\n",
        "                      y='E4vE3_Fold_change',\n",
        "                      data=All_genotype_Fold_changes,\n",
        "                      legend=False,\n",
        "                      palette='cividis',\n",
        "                      s = 350,\n",
        "                      alpha=0.8)\n",
        "  return(VP)\n",
        "\n",
        "plotVP(All_genotype_Fold_changes)\n"
      ],
      "metadata": {
        "id": "mWfdRKTNZzeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primary_E2vE3_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E2vE3_primary_PVFC.csv',index=True)\n",
        "primary_E4vE3_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE3_primary_PVFC.csv',index=True)\n",
        "primary_E4vE2_stats.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE2_primary_PVFC.csv',index=True)"
      ],
      "metadata": {
        "id": "rC67KYsB8qdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primary_E2vE3_stats = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E2vE3_primary_PVFC.csv').set_index('Accession')\n",
        "primary_E4vE3_stats = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE3_primary_PVFC.csv').set_index('Accession')\n",
        "primary_E4vE2_stats = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/primary_PVFC/E4vE2_primary_PVFC.csv').set_index('Accession')"
      ],
      "metadata": {
        "id": "-2jSIWc30DZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['#306998','#646464','#FFD43B']\n",
        "custom_pal = sns.color_palette(colors)\n",
        "\n",
        "change_cutoff = np.log2(1.5)\n",
        "\n",
        "def plotVP(input_df,\n",
        "           plot_title=None,\n",
        "           FC_column_name=None,\n",
        "           PV_column_name=None,\n",
        "           fractions=None,\n",
        "           comparison=None,\n",
        "           style=None):\n",
        "\n",
        "  df = input_df.copy()\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "\n",
        "  custom_params = {'axes.linewidth': 3}\n",
        "\n",
        "  sns.set(style='ticks', context='paper',font='sans-serif', font_scale=1, color_codes=True,rc=custom_params)\n",
        "\n",
        "  plt.axvline(x=change_cutoff, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "  plt.axvline(x=-change_cutoff, ymin=0, ymax=1, linestyle=':',color='gray')\n",
        "  plt.axhline(y=significance_cutoff, xmin=0, xmax=1, linestyle=':',color='gray')\n",
        "  plt.xlabel(\"log2(fold change)\", fontweight ='bold', size=20)\n",
        "  plt.ylabel(\"-10Log(BH-corrected p-value)\", fontweight ='bold',size=20)\n",
        "  # plt.title(f'{plot_title} P-value vs. Fold Change',fontsize=30)\n",
        "  plt.tick_params(axis='both', which='major', labelsize=15,length = 5,color = 'black',width =3)\n",
        "\n",
        "  plt.xlim(-4,4)\n",
        "\n",
        "  df['cutoff'] = ''\n",
        "\n",
        "  sigUp_count = len(df.loc[((df[FC_column_name] >= change_cutoff)) & (df[PV_column_name] >= significance_cutoff),['cutoff']].index)\n",
        "  sigDown_count = len(df.loc[((df[FC_column_name] <= -change_cutoff)) & (df[PV_column_name] >= significance_cutoff),['cutoff']].index)\n",
        "  no_change1 = len(df.loc[((df[FC_column_name] < change_cutoff) | (df[FC_column_name] > -change_cutoff)) & (df[PV_column_name] < significance_cutoff),['cutoff']].index)\n",
        "  no_change2 = len(df.loc[((df[FC_column_name] < change_cutoff) | (df[FC_column_name] > -change_cutoff)) & (df[PV_column_name] >= significance_cutoff),['cutoff']].index)\n",
        "\n",
        "  no_change_total = (no_change1 + no_change2) - (sigUp_count + sigDown_count)\n",
        "\n",
        "  print(sigUp_count,sigDown_count,no_change1)\n",
        "\n",
        "  df.loc[((df[FC_column_name] < change_cutoff) | (df[FC_column_name] > -change_cutoff)) & (df[PV_column_name] < significance_cutoff),['cutoff']] = f\"━{str(no_change_total)}\"\n",
        "  df.loc[((df[FC_column_name] < change_cutoff) | (df[FC_column_name] > -change_cutoff)) & (df[PV_column_name] >= significance_cutoff),['cutoff']] = f\"━{str(no_change_total)}\"\n",
        "  df.loc[((df[FC_column_name] >= change_cutoff)) & (df[PV_column_name] >= significance_cutoff),['cutoff']] = f\"⬆{str(sigUp_count)}\"\n",
        "  df.loc[((df[FC_column_name] <= -change_cutoff)) & (df[PV_column_name] >= significance_cutoff),['cutoff']] = f\"⬇{str(sigDown_count)}\"\n",
        "\n",
        "  display(df)\n",
        "\n",
        "  markers = {\"BM\": \"p\", \"BC\": \"o\"}\n",
        "\n",
        "  VP = sns.scatterplot(x=FC_column_name,\n",
        "                      y=PV_column_name,\n",
        "                      data=df,\n",
        "                      legend=True,\n",
        "                      palette=custom_pal,\n",
        "                      hue='cutoff',\n",
        "                      s = 200,\n",
        "                      alpha=0.9,\n",
        "                      edgecolor=\"black\",\n",
        "                      linewidth=0.8,\n",
        "                      hue_order = [f\"⬆{str(sigUp_count)}\",f\"━{str(no_change_total)}\",f\"⬇{str(sigDown_count)}\"],\n",
        "                      markers = markers,\n",
        "                      style=style)\n",
        "\n",
        "  plt.legend(loc=1, prop={'size':15},title=None,title_fontsize=20,markerscale=3)\n",
        "  sns.despine(top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
        "  plt.savefig(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{sex}/Figures/{sex}_{comparison}_{fractions}_primary_VP.svg',format=\"svg\",transparent=True)\n",
        "\n",
        "  return(VP)"
      ],
      "metadata": {
        "id": "Ci1n1wxi3xF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_scaling(df):\n",
        "  df_mean = np.mean(df[f'Fold_change'])\n",
        "  df_std = np.std(df[f'Fold_change'])\n",
        "  df[f'Fold_change'] = df[f'Fold_change'] - df_mean\n",
        "  df[f'Fold_change'] = df[f'Fold_change'] / df_std\n",
        "  return(df)\n",
        "\n",
        "def range_scaling(df):\n",
        "  df_min = np.min(df[f'Fold_change'])\n",
        "  df_max = np.max(df[f'Fold_change'])\n",
        "  df_mean = np.mean(df[f'Fold_change'])\n",
        "  df[f'Fold_change'] = df[f'Fold_change'] - df_mean\n",
        "  df[f'Fold_change'] = df[f'Fold_change'] / (df_max - df_min)\n",
        "  df[f'Fold_change'] = df[f'Fold_change']*10\n",
        "  return(df)\n",
        "\n",
        "def pareto_scaling(df):\n",
        "  df_mean = np.mean(df[f'Fold_change'])\n",
        "  df_std = np.std(df[f'Fold_change'])\n",
        "  df[f'Fold_change'] = df[f'Fold_change'] - df_mean\n",
        "  df[f'Fold_change'] = df[f'Fold_change'] /  np.sqrt(df_std)\n",
        "  return(df)\n",
        "\n",
        "def vast_scaling(df):\n",
        "  Rate_FC_DF_mean = np.mean(Rate_FC_DF[f'{comparison}_rate_FC'])\n",
        "  Rate_FC_DF_std = np.std(Rate_FC_DF[f'{comparison}_rate_FC'])\n",
        "  Rate_FC_DF[f'{comparison}_rate_FC'] = Rate_FC_DF[f'{comparison}_rate_FC'] - Rate_FC_DF_mean\n",
        "  Rate_FC_DF[f'{comparison}_rate_FC'] = Rate_FC_DF[f'{comparison}_rate_FC'] / (Rate_FC_DF_std)\n",
        "  Rate_FC_DF[f'{comparison}_rate_FC'] = Rate_FC_DF[f'{comparison}_rate_FC'] * (Rate_FC_DF_mean/Rate_FC_DF_std)\n",
        "\n",
        "def level_scaling(df):\n",
        "  Rate_FC_DF_mean = np.mean(Rate_FC_DF[f'{comparison}_rate_FC'])\n",
        "  Rate_FC_DF[f'{comparison}_rate_FC'] = Rate_FC_DF[f'{comparison}_rate_FC'] - Rate_FC_DF_mean\n",
        "  Rate_FC_DF[f'{comparison}_rate_FC'] = Rate_FC_DF[f'{comparison}_rate_FC'] / (Rate_FC_DF_mean)"
      ],
      "metadata": {
        "id": "p9CdudkF7V6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('primary_E2vE3_stats')\n",
        "print(len(primary_E2vE3_stats.index))\n",
        "plotVP(primary_E2vE3_stats,\n",
        "           plot_title='Test',\n",
        "           FC_column_name='Fold_change',\n",
        "           PV_column_name='-10*log10(BH)',\n",
        "           fractions='BCM',\n",
        "           comparison='E2vE3',\n",
        "           style=None)\n",
        "\n",
        "print('primary_E4vE3_stats')\n",
        "print(len(primary_E4vE3_stats.index))\n",
        "plotVP(primary_E4vE3_stats,\n",
        "           plot_title='Test',\n",
        "           FC_column_name='Fold_change',\n",
        "           PV_column_name='-10*log10(BH)',\n",
        "           fractions='BCM',\n",
        "           comparison='E4vE3',\n",
        "           style=None)\n",
        "\n",
        "print('primary_E4vE2_stats')\n",
        "print(len(primary_E4vE2_stats.index))\n",
        "plotVP(primary_E4vE2_stats,\n",
        "           plot_title='Test',\n",
        "           FC_column_name='Fold_change',\n",
        "           PV_column_name='-10*log10(BH)',\n",
        "           fractions='BCM',\n",
        "           comparison='E4vE2',\n",
        "           style=None)"
      ],
      "metadata": {
        "id": "RZ92CASC-4Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prepare the dataframe with the fold changes for each protein in each comparison\n",
        "comparison_DF = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{FC_sex}/primary_PVFC/{comparison}_primary_PVFC.csv').set_index('Accession').rename(columns={'Fold_change':f'{comparison}_Fold_change'}).filter(regex=f'{comparison}|BH')\n",
        "\n",
        "# Scale Fold Change Data using Range Scaling\n",
        "comparison_DF_min = np.min(comparison_DF[f'{comparison}_Fold_change'])\n",
        "comparison_DF_max = np.max(comparison_DF[f'{comparison}_Fold_change'])\n",
        "comparison_DF_mean = np.mean(comparison_DF[f'{comparison}_Fold_change'])\n",
        "comparison_DF[f'{comparison}_Fold_change'] = comparison_DF[f'{comparison}_Fold_change'] - comparison_DF_mean\n",
        "comparison_DF[f'{comparison}_Fold_change'] = comparison_DF[f'{comparison}_Fold_change'] / (comparison_DF_max - comparison_DF_min)\n",
        "\n",
        "# Reverser Log2 the data for calculations\n",
        "comparison_DF[f'{comparison}_Fold_change'] = 2**comparison_DF[f'{comparison}_Fold_change']\n",
        "\n",
        "# Create a list that contains the protein ID used to match the protein names in the String output\n",
        "protein_list = comparison_DF.index.to_list()\n",
        "for position,protein in enumerate(protein_list):\n",
        "  protein = protein.split('|')[0]\n",
        "  protein_list[position] = protein\n",
        "\n",
        "comparison_DF['Accession'] = protein_list\n",
        "\n",
        "comparison_DF = comparison_DF.set_index('Accession')\n",
        "\n",
        "full_comparison_DF = full_comparison_DF[[f'{comparison}_Fold_change','Protein_ID','Accession']].dropna()\n",
        "\n",
        "STRING_ID_MAP = pd.read_csv('/content/drive/MyDrive/ApoE Analysis January-11-2023/String_ID_Map.tsv', sep='\\t').rename(columns={'From':'Accession','To':'Protein_ID'})\n",
        "STRING_ID_MAP = STRING_ID_MAP.sort_values(by='Accession')\n",
        "STRING_ID_MAP = STRING_ID_MAP.drop_duplicates('Accession').set_index('Accession')\n",
        "\n",
        "full_comparison_DF = pd.concat([full_comparison_DF,STRING_ID_MAP],axis=1).dropna()\n",
        "\n",
        "# Read string DB output CSV\n",
        "string_annot = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/String All Genotype Analysis.tsv',sep='\\t')\n",
        "string_annot['matching proteins in your network (labels)'] = string_annot['matching proteins in your network (labels)'][:].str.split(r',')\n",
        "\n",
        "string_annot['Ontology_coverage (%)'] = string_annot['observed gene count']/string_annot['background gene count']\n",
        "string_annot = string_annot.loc[string_annot['Ontology_coverage (%)'] >= 0.25,]\n",
        "\n",
        "ontologies_of_interest = ['GO Process','GO Function','GO Component','KEGG','Reactome','WikiPathways']\n",
        "string_annot = string_annot.loc[string_annot['#category'].isin(ontologies_of_interest)].reset_index(drop=True)\n",
        "\n",
        "string_annot[[f'{comparison}']] = None\n",
        "string_annot[f'measurement_count'] = None\n",
        "string_annot[f'sig_proteins_in_ont'] = None\n",
        "string_annot[f'sig_proteins_count'] = None\n",
        "\n",
        "\n",
        "# Create a dataframe column that contains all protein fold changes from the ontology\n",
        "counter = 0\n",
        "while counter < len(string_annot.index):\n",
        "  protein_list = string_annot.loc[counter,'matching proteins in your network (labels)']\n",
        "  protein_in_ontology = full_comparison_DF.loc[full_comparison_DF['Protein_ID'].isin(protein_list)]\n",
        "  string_annot[f'{comparison}'][counter] = (protein_in_ontology[f'{comparison}_Fold_change'].values.tolist())\n",
        "  string_annot.loc[counter,f'measurement_count'] = len((protein_in_ontology[f'{comparison}_Fold_change'].values.tolist()))\n",
        "  counter += 1\n",
        "\n",
        "# Create a column with a list of proteins that have significant change in concentration\n",
        "sig_proteins = full_comparison_DF.loc[full_comparison_DF['-10*log10(BH)'] > np.log10(0.05)*-10, ]\n",
        "\n",
        "counter = 0\n",
        "while counter < len(string_annot.index):\n",
        "\n",
        "  protein_list = string_annot.loc[counter,'matching proteins in your network (labels)']\n",
        "  sig_protein_in_ontology = sig_proteins.loc[sig_proteins['Protein_ID'].isin(protein_list)].index.to_list()\n",
        "  string_annot[f'sig_proteins_in_ont'][counter] = sig_protein_in_ontology\n",
        "  string_annot.loc[counter,f'sig_proteins_count'] = len(sig_protein_in_ontology)\n",
        "  counter += 1\n",
        "\n",
        "string_annot[f'sig_proteins_ont_coverage'] = string_annot['sig_proteins_count']/string_annot['background gene count']\n",
        "\n",
        "# Create a column for average ontology fold change and p-value calculated from all protein fold changes.\n",
        "string_annot[f'{comparison}_PV_from_Ontology_FC'] = None\n",
        "string_annot[f'{comparison}_Avg_Ontology_FC'] = None\n",
        "\n",
        "# Use a one-sample t-test for fold changes in the ontology (log2(FC) != 1) and calculate FC average for the ontology\n",
        "counter = 0\n",
        "while counter < len(string_annot.index):\n",
        "  if len(string_annot[f'{comparison}'][counter]) > 1:\n",
        "\n",
        "    string_annot.loc[counter,f'{comparison}_Avg_Ontology_FC'] = np.mean(string_annot[f'{comparison}'][counter])\n",
        "    string_annot.loc[counter,f'{comparison}_PV_from_Ontology_FC'] = st.ttest_1samp((string_annot[f'{comparison}'][counter]), 1 , nan_policy='omit', alternative='two-sided')[1]\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter += 1\n",
        "\n",
        "# Log transform FCs and PVs\n",
        "string_annot[f'{comparison}_log_PV'] = np.log10(string_annot[f'{comparison}_PV_from_Ontology_FC'].astype(float))*-10\n",
        "string_annot[f'{comparison}_log_FC'] = np.log2(string_annot[f'{comparison}_Avg_Ontology_FC'].astype(float))*10\n",
        "\n",
        "string_annot = string_annot.dropna(subset=[f'{comparison}_log_PV'])\n",
        "string_annot = string_annot.sort_values(by=f'{comparison}_PV_from_Ontology_FC')\n",
        "string_annot = string_annot.drop_duplicates(subset=['term ID'], keep='first')\n",
        "\n",
        "# Calculate BH P-value correction\n",
        "def performBH_correction(input_df,comparison=None):\n",
        "  input_df[f'{comparison}_BH_from_Ontology_FC'] = None\n",
        "  input_df = input_df.loc[input_df[f'{comparison}_log_PV'] > 0,]\n",
        "  input_df = input_df.reset_index()\n",
        "  #sort cleaned_df by pvalue jc mod\n",
        "  input_df = input_df.sort_values(by=f'{comparison}_PV_from_Ontology_FC')\n",
        "  input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "  #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "  total_rows = len(input_df.index)\n",
        "  for row in input_df.itertuples():\n",
        "    BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "    input_df.at[row.Index, f'{comparison}_BH_from_Ontology_FC'] = BH_pval\n",
        "  input_df[f'{comparison}_-10*log10(BH)'] = np.log10(input_df[f'{comparison}_BH_from_Ontology_FC'].astype(float))*-10\n",
        "  return(input_df)\n",
        "\n",
        "Ontology_Comparison = performBH_correction(string_annot,comparison=f'{comparison}')\n",
        "\n",
        "Ontology_Comparison.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/Male-Female/String_Ontology_Stats/{comparison}_SDB_Stats.csv')\n",
        "\n",
        "Ontology_Comparison"
      ],
      "metadata": {
        "id": "nGivJdEAi7id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def create_Rate_DF(comparison=comparison):\n",
        "\n",
        "  protein_ID_guide = pd.read_csv('/content/drive/MyDrive/ApoE Turnover Data/Protein_ID_Guide/Protein_ID_Guide_File.csv').set_index('Accession')\n",
        "\n",
        "  # Create dataframes for rates in each isoform\n",
        "  a2_df = pd.DataFrame()\n",
        "  a3_df = pd.DataFrame()\n",
        "  a4_df = pd.DataFrame()\n",
        "\n",
        "  # Where are the Rate files located\n",
        "  where_are_the_files = f'/content/drive/MyDrive/ApoE Turnover Data/Female_Rates_Peptide_FDR_2'\n",
        "  os.chdir(where_are_the_files)\n",
        "  all_file_names = [i for i in glob.glob('*.{}'.format('csv'))]\n",
        "\n",
        "  full_bkg_df = pd.DataFrame()\n",
        "\n",
        "  # Filter each rate file and create allele specific dataframes\n",
        "  for dataSet in all_file_names:\n",
        "    fileName = dataSet.strip('.csv')\n",
        "    TR_df = pd.read_csv(dataSet).set_index('analyte_id').filter(regex='Combined')\n",
        "\n",
        "    TR_df = TR_df.fillna(0)\n",
        "    TR_df = TR_df.loc[TR_df['Combined uniques'].astype(int) > 1,]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'] != 'Insufficient Timepoints',]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'] != 'value could not be determined',]\n",
        "    TR_df = TR_df.loc[TR_df['Combined rate'].astype(float) > 0,]\n",
        "    TR_df = TR_df.loc[TR_df['Combined R2'].astype(float) > 0.6,]\n",
        "    TR_df = TR_df.filter(regex='rate')\n",
        "\n",
        "    allele = re.findall('A\\d|M\\d', fileName)[0]\n",
        "    TR_df = TR_df.rename(columns={'Combined rate' : f'{allele}'})\n",
        "    if allele == 'A2':\n",
        "      a2_df = pd.concat([a2_df,TR_df])\n",
        "    elif allele == 'A3':\n",
        "      a3_df = pd.concat([a3_df,TR_df])\n",
        "    elif allele == 'A4':\n",
        "      a4_df = pd.concat([a4_df,TR_df])\n",
        "\n",
        "\n",
        "  # Calculate the average rate for each protein from the different rate files\n",
        "  a2_df['A2'] = a2_df['A2'].astype(float)\n",
        "  a2_df = a2_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a2_std = np.std(np.log2(a2_df['A2']))\n",
        "  a2_df['A2'] = np.log2(a2_df['A2'])\n",
        "\n",
        "  a3_df['A3'] = a3_df['A3'].astype(float)\n",
        "  a3_df = a3_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a3_std = np.std(np.log2(a3_df['A3']))\n",
        "  a3_df['A3'] = np.log2(a3_df['A3'])\n",
        "\n",
        "  a4_df['A4'] = a4_df['A4'].astype(float)\n",
        "  a4_df = a4_df.groupby(by='analyte_id',as_index=True).mean()\n",
        "  a4_std = np.std(np.log2(a4_df['A4']))\n",
        "  a4_df['A4'] = np.log2(a4_df['A4'])\n",
        "\n",
        "  # Create a dataframe with all the rates and\n",
        "  all_rates = pd.concat([a2_df,a3_df,a4_df,protein_ID_guide],axis=1)\n",
        "\n",
        "  # Create comparison specific dataframes\n",
        "  E2vE3_rates_df = pd.concat([a2_df,a3_df,protein_ID_guide],axis=1).dropna()\n",
        "  E4vE3_rates_df = pd.concat([a3_df,a4_df,protein_ID_guide],axis=1).dropna()\n",
        "  E4vE2_rates_df = pd.concat([a2_df,a4_df,protein_ID_guide],axis=1).dropna()\n",
        "\n",
        "  # Calculate fold change\n",
        "  E2vE3_rates_df['E2vE3_rate_FC'] = ((E2vE3_rates_df['A2'] - E2vE3_rates_df['A3']) )\n",
        "  E4vE3_rates_df['E4vE3_rate_FC'] = ((E4vE3_rates_df['A4'] - E4vE3_rates_df['A3']) )\n",
        "  E4vE2_rates_df['E4vE2_rate_FC'] = ((E4vE2_rates_df['A4'] - E4vE2_rates_df['A2']) )\n",
        "\n",
        "  # Concatenate the the comparison dataframes\n",
        "  Rate_FC_DF = pd.concat([E2vE3_rates_df['E2vE3_rate_FC'],E4vE3_rates_df['E4vE3_rate_FC'],E4vE2_rates_df['E4vE2_rate_FC'],protein_ID_guide],axis=1)\n",
        "\n",
        "  # norm = Rate_FC_DF.plot.density(figsize=(10, 10),)\n",
        "  # plt.title('Rate Fold Change Density Plot')\n",
        "  # plt.xlim(-6,6)\n",
        "\n",
        "  Rate_FC_DF.to_csv('/content/drive/MyDrive/ApoE Turnover Data/Rate_FC_DF')\n",
        "\n",
        "  # full_Rate_FC_df = Rate_FC_DF.filter(regex=f'{comparison}|ID')\n",
        "  full_Rate_FC_df = Rate_FC_DF\n",
        "\n",
        "  print('This function outputs a rate FC dataframe; the FC was calculated with the log2 rate values, FC = [log2(B) - log(A)]')\n",
        "  # display(full_Rate_FC_df)\n",
        "\n",
        "  return(full_Rate_FC_df)\n",
        "\n",
        "full_Rate_FC_df = create_Rate_DF(comparison=comparison)\n",
        "\n",
        "Rate_FC_df = full_Rate_FC_df.copy()\n",
        "Rate_FC_df = Rate_FC_df.filter(regex=f'{comparison}|ID').dropna()\n",
        "\n",
        "def auto_scaling(df):\n",
        "  Rate_FC_DF_mean = np.mean(df[f'{comparison}_rate_FC'])\n",
        "  Rate_FC_DF_std = np.std(df[f'{comparison}_rate_FC'])\n",
        "  df[f'{comparison}_rate_FC'] = df[f'{comparison}_rate_FC'] - Rate_FC_DF_mean\n",
        "  df[f'{comparison}_rate_FC'] = df[f'{comparison}_rate_FC'] / np.sqrt(Rate_FC_DF_std)\n",
        "  return(df)\n",
        "\n",
        "scaled_RDF = auto_scaling(Rate_FC_df).filter(regex='rate')\n",
        "\n",
        "STRING_ID_MAP = pd.read_csv('/content/drive/MyDrive/ApoE Analysis January-11-2023/String_ID_Map.tsv', sep='\\t').rename(columns={'From':'Accession','To':'Protein_ID'})\n",
        "STRING_ID_MAP = STRING_ID_MAP.sort_values(by='Accession')\n",
        "STRING_ID_MAP = STRING_ID_MAP.drop_duplicates('Accession').set_index('Accession')\n",
        "\n",
        "scaled_RDF =scaled_RDF.copy()\n",
        "\n",
        "scaled_RDF = pd.concat([scaled_RDF,STRING_ID_MAP],axis=1).dropna()\n",
        "\n",
        "full_Rate_FC_df = scaled_RDF.copy()\n",
        "\n",
        "# Format the string file to create readable proteins lists for each ontology\n",
        "string_annot = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/String All Genotype Analysis.tsv',sep='\\t')\n",
        "string_annot['matching proteins in your network (labels)'] = string_annot['matching proteins in your network (labels)'][:].str.split(r',')\n",
        "\n",
        "string_annot['Ontology_coverage (%)'] = string_annot['observed gene count']/string_annot['background gene count']\n",
        "string_annot = string_annot.loc[string_annot['Ontology_coverage (%)'] >= 0.25,]\n",
        "\n",
        "ontologies_of_interest = ['GO Process','GO Function','GO Component','KEGG','Reactome','WikiPathways']\n",
        "string_annot = string_annot.loc[string_annot['#category'].isin(ontologies_of_interest)].reset_index(drop=True)\n",
        "string_annot[[f'{comparison}_Rate']] = None\n",
        "\n",
        "# Create a dataframe column that contains all protein fold changes from the ontology\n",
        "counter = 0\n",
        "while counter < len(string_annot.index):\n",
        "  protein_list = string_annot.loc[counter,'matching proteins in your network (labels)']\n",
        "  protein_in_ontology = full_Rate_FC_df.loc[full_Rate_FC_df['Protein_ID'].isin(protein_list)]\n",
        "  string_annot[f'{comparison}_Rate'][counter] = (protein_in_ontology[f'{comparison}_rate_FC'].dropna()).values.tolist()\n",
        "  counter += 1\n",
        "\n",
        "# Create a column for average ontology fold change and p-value calculated from all protein fold changes.\n",
        "counter = 0\n",
        "string_annot[f'{comparison}_PV_from_Ontology_Rate_FC'] = None\n",
        "string_annot[f'{comparison}_PV_from_Ontology_Rate_FC'] = None\n",
        "\n",
        "string_annot[f'{comparison}_Avg_Ontology_Rate_FC'] = None\n",
        "string_annot[f'{comparison}_Avg_Ontology_Rate_FC'] = None\n",
        "\n",
        "# Use a one-sample t-test for fold changes in the ontology (FC != 1) and calculate FC average for the ontology\n",
        "counter = 0\n",
        "while counter < len(string_annot.index):\n",
        "  if len(string_annot[f'{comparison}_Rate'][counter]) > 2:\n",
        "    string_annot.loc[counter,f'{comparison}_PV_from_Ontology_Rate_FC'] = st.ttest_1samp((string_annot[f'{comparison}_Rate'][counter]), 0 , nan_policy='omit', alternative='two-sided')[1]\n",
        "    string_annot.loc[counter,f'{comparison}_Avg_Ontology_Rate_FC'] = np.mean(string_annot[f'{comparison}_Rate'][counter])\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter += 1\n",
        "\n",
        "rate_string_analysis = string_annot.copy()\n",
        "\n",
        "rate_string_analysis[f'{comparison}_Rate_log_PV'] = np.log10(rate_string_analysis[f'{comparison}_PV_from_Ontology_Rate_FC'].astype(float))*-10\n",
        "rate_string_analysis[f'{comparison}_Rate_log_FC'] = (rate_string_analysis[f'{comparison}_Avg_Ontology_Rate_FC'].astype(float))\n",
        "\n",
        "rate_string_analysis = rate_string_analysis.dropna(subset=[f'{comparison}_Rate_log_PV'])\n",
        "rate_string_analysis = rate_string_analysis.sort_values(by=f'{comparison}_PV_from_Ontology_Rate_FC')\n",
        "\n",
        "# Calculate BH P-value correction\n",
        "def performBH_correction(input_df,comparison=None):\n",
        "  input_df[f'{comparison}_BH_from_Ontology_Rate_FC'] = None\n",
        "  input_df = input_df.loc[input_df[f'{comparison}_Rate_log_PV'] > 0,]\n",
        "  input_df = input_df.reset_index()\n",
        "  input_df = input_df.sort_values(by=f'{comparison}_PV_from_Ontology_Rate_FC')\n",
        "  input_df = input_df.reset_index(drop=True) #sort keeps the origional index value so you need to re-index to use it in the BH calc\n",
        "  #calculate benjamini_hochberg correction as (rank/total numer of tests)*probability of false positive jc mod\n",
        "  total_rows = len(input_df.index)\n",
        "  for row in input_df.itertuples():\n",
        "    BH_pval = ((row.Index+1)/total_rows)*0.25\n",
        "    input_df.at[row.Index, f'{comparison}_BH_from_Ontology_Rate_FC'] = BH_pval\n",
        "  input_df[f'{comparison}_Rate_-10*log10(BH)'] = np.log10(input_df[f'{comparison}_BH_from_Ontology_Rate_FC'].astype(float))*-10\n",
        "  return(input_df)\n",
        "\n",
        "rate_string_analysis = performBH_correction(rate_string_analysis,comparison=f'{comparison}')\n",
        "\n",
        "rate_string_analysis.to_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/Male-Female/String_Ontology_Stats/{comparison}_SDB_Rate_Stats.csv')"
      ],
      "metadata": {
        "id": "7ncgXszQjYqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "List_Dictionary_E2vE3 = {'Mitochondrial Components':['GO:0031966','GO:0005743','GO:1990542','GO:0098798','GO:0030150','GO:0005759'],\n",
        "                         'OXPHOS':['MMU-1428517','GO:0022900','GO:0033177','MMU-204174','GO:0005967','mmu00020'],\n",
        "                         'Proteasome':['GO:0005839'],\n",
        "                         'Endolysosomal Pathway':['GO:0005765','GO:0005764','GO:0007041','GO:0007040']\n",
        "                         }\n",
        "\n",
        "\n",
        "List_Dictionary_E4vE3 = {'Mitochondrial Components':['GO:0031966','GO:0005743','GO:1990542','GO:0098573','GO:0098798','GO:0031305','GO:0005742','GO:0098800'],\n",
        "                         'OXPHOS':['GO:0022904','MMU-1428517','GO:0009060','mmu00051'],\n",
        "                         'Proteasome':['GO:0030544','GO:0008541','GO:0005838','GO:0022624','GO:0000502'],\n",
        "                         'Endolysosomal Pathway':['GO:0005765','GO:0005764','MMU-8856828','GO:0005770','GO:0031902']\n",
        "                         }"
      ],
      "metadata": {
        "id": "Pd3flme-kSgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code concatenates the rate string analysis and the concentration string analysis\n",
        "\n",
        "def create_RNC_DF(comparison=None):\n",
        "\n",
        "  # Read the concentration SBD analysis\n",
        "  SDB_conc_Stats = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/Male-Female/String_Ontology_Stats/{comparison}_SDB_Stats.csv')\n",
        "  SDB_conc_Stats['Ontology_coverage (%)'] = SDB_conc_Stats['observed gene count']/SDB_conc_Stats['background gene count']\n",
        "  SDB_conc_Stats = SDB_conc_Stats.set_index('term ID').filter(regex='E\\dvE\\d_log_FC|term.description|PV|labels|observed|BH|coverage|sig').rename(columns={f'{comparison}' : f'{comparison}_FCs',f'{comparison}_log_FC' : f'{comparison}_Conc_log_FC'})\n",
        "\n",
        "  # Read the Rate SBD analysis\n",
        "  SDB_rate_Stats = pd.read_csv(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/Male-Female/String_Ontology_Stats/{comparison}_SDB_Rate_Stats.csv')\n",
        "  SDB_rate_Stats = SDB_rate_Stats.set_index('term ID').filter(regex=f'{comparison}_Rate_log_FC|category')\n",
        "  SDB_rate_Stats[f'{comparison}_Rate_log_FC'] = SDB_rate_Stats[f'{comparison}_Rate_log_FC']\n",
        "\n",
        "  # Combined the Concentration and Rate SDB analysis\n",
        "  RnC_Df = pd.concat([SDB_conc_Stats,SDB_rate_Stats],axis=1).dropna()\n",
        "  RnC_Df = RnC_Df.rename(columns={'observed gene count':'Ontology Protein Count'})\n",
        "  RnC_Df[\"term description (Count)\"] = (RnC_Df['term description']).astype(str) + \" \" +\"(\" + (RnC_Df['Ontology Protein Count'].astype(int)).astype(str) + \")\"\n",
        "  RnC_Df = RnC_Df.filter(regex='E\\dvE\\d_log_FC|term.description.|labels|observed|10.BH|coverage|log_FC|count|category|sig')\n",
        "\n",
        "  RnC_Df = RnC_Df.rename(columns={f'sig_proteins_in_ont':f'{comparison}_sig_proteins_in_ont'})\n",
        "  RnC_Df = RnC_Df.rename(columns={f'sig_proteins_count':f'{comparison}_sig_proteins_count'})\n",
        "\n",
        "  RnC_Df = RnC_Df[['#category',\n",
        "                  'term description (Count)',\n",
        "                  f'{comparison}_Conc_log_FC',\n",
        "                  f'{comparison}_Rate_log_FC',\n",
        "                  f'{comparison}_-10*log10(BH)',\n",
        "                  'Ontology_coverage (%)',\n",
        "                  'matching proteins in your network (labels)',\n",
        "                  f'{comparison}_sig_proteins_in_ont',\n",
        "                  f'{comparison}_sig_proteins_count']]\n",
        "\n",
        "\n",
        "  return(RnC_Df)\n",
        "\n",
        "RnC_Df_for_lists_E2vE3 = create_RNC_DF(comparison='E2vE3')\n",
        "RnC_Df_for_lists_E4vE3 = create_RNC_DF(comparison='E4vE3')\n",
        "RnC_Df_for_lists_E4vE2 = create_RNC_DF(comparison='E4vE2')"
      ],
      "metadata": {
        "id": "XehYeBBdkfF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# colors = ['#646464','#306998','#FFD43B']\n",
        "colors = ['#00375F','#730707']\n",
        "custom_pal = sns.color_palette(colors)\n",
        "\n",
        "def plot_Quadrants(input_df,\n",
        "           plot_title=None,\n",
        "           Conc_column_name=None,\n",
        "           Rate_column_name=None,\n",
        "           fractions=None,\n",
        "           comparison=None,\n",
        "           style=None,\n",
        "           list_of_interest=None,\n",
        "           Ontology_name=None):\n",
        "\n",
        "  df = input_df.copy().filter(regex='log.FC|term|labels|observed|BH')\n",
        "\n",
        "  plt.figure(figsize=(3,3))\n",
        "\n",
        "  custom_params = {'axes.linewidth': 1}\n",
        "\n",
        "  sns.set(style='ticks', context='paper', color_codes=True,rc=custom_params)\n",
        "  # sns.set(style='ticks', context='paper', font_scale=1, color_codes=True,rc=custom_params)\n",
        "\n",
        "  plt.axvline(x=0, ymin=0, ymax=1, linestyle=':',color='gray',linewidth=2)\n",
        "  plt.axhline(y=0, xmin=0, xmax=1, linestyle=':',color='gray',linewidth=2)\n",
        "  plt.xlabel(\"ΔConcentration (arb. unit)\", fontweight ='bold', size=10)\n",
        "  plt.ylabel(\"ΔRate (arb. unit)\", fontweight ='bold',size=10)\n",
        "  plt.title(f'{comparison} Ontology ΔConcentration vs ΔRate',fontweight ='bold',size=10, pad=5, wrap=False)\n",
        "  plt.tick_params(axis='both', which='major', labelsize=10,length = 2,color = 'black',width =1)\n",
        "\n",
        "  df[\"significance\"] = \"Insignificant\"\n",
        "  df.loc[(df[f'{comparison}_-10*log10(BH)'] > np.log10(0.025)*-10), 'significance'] = \"Significant\"\n",
        "  df.sort_values(by=f'{comparison}_-10*log10(BH)',inplace=True)\n",
        "\n",
        "  sig_len = len(df.loc[(df[\"significance\"] == 'Significant'),])\n",
        "  insig_len = len(df.loc[(df[\"significance\"] == 'Insignificant'),])\n",
        "\n",
        "  df[\"significance\"] = f\"Insignificant ({insig_len})\"\n",
        "  df.loc[(df[f'{comparison}_-10*log10(BH)'] > np.log10(0.025)*-10), 'significance'] = f\"Significant ({sig_len})\"\n",
        "  df.sort_values(by=f'{comparison}_-10*log10(BH)',inplace=True)\n",
        "\n",
        "\n",
        "  VP = sns.scatterplot(x=f'{comparison}_Conc_log_FC',\n",
        "                      y=f'{comparison}_Rate_log_FC',\n",
        "                      data=df,\n",
        "                      legend=True,\n",
        "                      palette=custom_pal,\n",
        "                      # c='#00375F',\n",
        "                      hue='significance',\n",
        "                      s = 50,\n",
        "                      alpha=0.7,\n",
        "                      edgecolor=\"black\",\n",
        "                      linewidth=0.4,\n",
        "                      hue_order = [f\"Insignificant ({insig_len})\",f\"Significant ({sig_len})\"],\n",
        "                      style=style)\n",
        "\n",
        "  plt.legend(loc=8, prop={'size':8},title_fontsize=10,markerscale=.75,borderpad=0.1,borderaxespad=-5, ncols=2)\n",
        "\n",
        "  # ax.legend(prop={'size':15},title_fontsize=20,markerscale=3,borderpad=0.5,borderaxespad=-5, ncols=2)\n",
        "\n",
        "  # ax.legend(prop={'size':15},title_fontsize=20,markerscale=3,borderpad=0.5,loc=8,borderaxespad=-5, ncols=2)\n",
        "\n",
        "  sns.despine(top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
        "\n",
        "  plt.savefig(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{FC_sex}/String_Ontology_Stats/{comparison}_Proteostasis_Plot.svg',format=\"svg\",transparent=True, dpi=1200,bbox_inches='tight')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return(VP)\n",
        "\n",
        "\n",
        "plot_Quadrants(RnC_Df_for_lists_E2vE3,\n",
        "              plot_title='Rate vs Concentration',\n",
        "              Conc_column_name=f'E2vE3_log_FC',\n",
        "              Rate_column_name=f'E2vE3_Rate_log_FC',\n",
        "              fractions=f'E2vE3',\n",
        "              comparison=f'E2vE3',\n",
        "              style=None,\n",
        "              list_of_interest=None,\n",
        "              Ontology_name=None)\n",
        "\n",
        "plot_Quadrants(RnC_Df_for_lists_E4vE3,\n",
        "              plot_title='Rate vs Concentration',\n",
        "              Conc_column_name=f'E4vE3_log_FC',\n",
        "              Rate_column_name=f'E4vE3_Rate_log_FC',\n",
        "              fractions=f'E4vE3',\n",
        "              comparison=f'E4vE3',\n",
        "              style=None,\n",
        "              list_of_interest=None,\n",
        "              Ontology_name=None)"
      ],
      "metadata": {
        "id": "seglcnk0kj3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This creates dictionary with Ontology category as the keys and the proteins associated with those ontology categories\n",
        "\n",
        "def Ontology_protein_dictionary(SDB_conc_Stats=None,comparison=None):\n",
        "  if comparison == 'E2vE3':\n",
        "    List_Dictionary = List_Dictionary_E2vE3\n",
        "  elif comparison == 'E4vE3':\n",
        "    List_Dictionary = List_Dictionary_E4vE3\n",
        "\n",
        "  SDB_conc_Stats = SDB_conc_Stats.reset_index()\n",
        "\n",
        "  All_Ontology_Genes_list = []\n",
        "\n",
        "  Ontology_protein_dictionary = {}\n",
        "\n",
        "  for ontology_category,ontologies in List_Dictionary.items():\n",
        "    # print(ontology_category)\n",
        "\n",
        "    Ontology_category_protein_list = []\n",
        "\n",
        "    for ontology in ontologies:\n",
        "      print(ontology)\n",
        "      ontology_proteins = SDB_conc_Stats.loc[SDB_conc_Stats['term ID'] == ontology,'matching proteins in your network (labels)'].values.tolist()[0]\n",
        "      # print(ontology_proteins)\n",
        "      ontology_proteins = ontology_proteins[:].strip(r'[|]')\n",
        "      ontology_proteins = ontology_proteins[:].replace(\" \", \"\")\n",
        "      ontology_proteins = ontology_proteins[:].replace(\"'\", \"\")\n",
        "      ontology_proteins = ontology_proteins[:].split(r',')\n",
        "      Ontology_category_protein_list = Ontology_category_protein_list + ontology_proteins\n",
        "\n",
        "    Proteins_in_ontology_category = list(dict.fromkeys(Ontology_category_protein_list))\n",
        "    Proteins_in_ontology_category = sorted(Proteins_in_ontology_category)\n",
        "    ontology_category_protein_count = len(Proteins_in_ontology_category)\n",
        "\n",
        "    All_Ontology_Genes_list = Proteins_in_ontology_category + All_Ontology_Genes_list\n",
        "\n",
        "    Ontology_protein_dictionary[ontology_category] = Proteins_in_ontology_category\n",
        "\n",
        "  all_Proteins_in_ontology_category = list(dict.fromkeys(All_Ontology_Genes_list))\n",
        "  all_Proteins_in_ontology_category = sorted(all_Proteins_in_ontology_category)\n",
        "  all_ontology_category_protein_count = len(all_Proteins_in_ontology_category)\n",
        "\n",
        "  return(Ontology_protein_dictionary)\n",
        "\n",
        "Ontology_protein_dictionary_E2vE3 = Ontology_protein_dictionary(SDB_conc_Stats=RnC_Df_for_lists_E2vE3,comparison='E2vE3')\n",
        "Ontology_protein_dictionary_E4vE3 = Ontology_protein_dictionary(SDB_conc_Stats=RnC_Df_for_lists_E4vE3,comparison='E4vE3')"
      ],
      "metadata": {
        "id": "im05497Jk2QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import wrap\n",
        "\n",
        "def bar_plot_for_ontologies(input_df,list_of_interest,Ontology_name=None,comparison=None):\n",
        "\n",
        "  df = input_df.copy()\n",
        "  df = df.reset_index()\n",
        "  df = df.set_index(df['term ID'])\n",
        "\n",
        "  ontologies_of_interest = df.loc[df['term ID'].isin(list(list_of_interest)),]\n",
        "  ontologies_of_interest = ontologies_of_interest.drop_duplicates(subset='term ID')\n",
        "  ontologies_of_interest = ontologies_of_interest.set_index('term description (Count)')\n",
        "  ontologies_of_interest.sort_values(by=f'{comparison}_Conc_log_FC', inplace=True, ascending=True)\n",
        "\n",
        "  first_bar = ontologies_of_interest[f'{comparison}_Conc_log_FC']\n",
        "  first_bar_label = 'ΔConcentration'\n",
        "  first_bar_color = '#EA7600'\n",
        "  second_bar = ontologies_of_interest[f'{comparison}_Rate_log_FC']\n",
        "  second_bar_label = 'ΔRate'\n",
        "  second_bar_color = '#236192'\n",
        "  labels = ontologies_of_interest.index\n",
        "  labels = [ '\\n'.join(wrap(l, 30)) for l in labels ]\n",
        "  width = 0.5  # the width of the bars\n",
        "  plot_title = f'{Ontology_name} Ontologies \\n ΔConcentration and ΔRate\\n{comparison}'\n",
        "  title_size = 10\n",
        "\n",
        "  # plt.suptitle(f'{Ontology_name}', y=1.05, fontsize=20)\n",
        "\n",
        "  # Sort values for plotting\n",
        "\n",
        "  # Plot figure\n",
        "  # fig, ax = plt.subplots(figsize=(10,15),linewidth=10)\n",
        "  fig, ax = plt.subplots(linewidth=2)\n",
        "\n",
        "  ax.tick_params(width=2)\n",
        "  plt_size = len(labels)*0.5\n",
        "  fig.set_figheight(plt_size+1)\n",
        "  # fig.set_figwidth(plt_size+1)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Plot double bars\n",
        "  y = np.arange(len(labels))  # the label locations\n",
        "  ax.barh(y + width/2, first_bar, width, label=first_bar_label, color=first_bar_color)\n",
        "  ax.barh(y - width/2, second_bar, width, label=second_bar_label, color=second_bar_color)\n",
        "\n",
        "  # Format ticks\n",
        "  ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.2f}'))\n",
        "\n",
        "  # Create labels\n",
        "  rects = ax.patches\n",
        "  for rect in rects:\n",
        "      # Get X and Y placement of label from rect.\n",
        "      x_value = rect.get_width()\n",
        "      y_value = rect.get_y() + rect.get_height() / 2\n",
        "      space = 1\n",
        "      ha = 'left'\n",
        "      if x_value < 0:\n",
        "          space *= 1\n",
        "          ha = 'right'\n",
        "      label = '{:,.2f}'.format(x_value)\n",
        "      plt.annotate(\n",
        "          label,\n",
        "          (x_value, y_value),\n",
        "          xytext=(space, 1),\n",
        "          textcoords='offset points',\n",
        "          va='center',\n",
        "          ha=ha,fontsize=10)\n",
        "\n",
        "  # plt.xticks(rotation=45)\n",
        "  # Set y-labels and legend\n",
        "\n",
        "  ax.set_yticklabels(labels)\n",
        "  ax.legend(prop={'size':10},title_fontsize=10,markerscale=3,borderpad=0.5,loc=8,borderaxespad=-5, ncols=2)\n",
        "\n",
        "  # To show each y-label, not just even ones\n",
        "  plt.yticks(np.arange(min(y), max(y)+1, 1.0))\n",
        "\n",
        "  # Adjust subplots\n",
        "  plt.margins(x=0.1)\n",
        "  plt.subplots_adjust(left=0.35, top=0.9)\n",
        "\n",
        "  plt.xlabel(\"Change (arb. units)\", fontweight ='bold', size=10)\n",
        "  plt.ylabel(\"Protein Ontology\\n(Protein Count)\", fontweight ='bold',size=10, wrap=False)\n",
        "  plt.tick_params(axis='both', which='major', labelsize=8,length = 2,color = 'black',width =2)\n",
        "  plt.tick_params(axis='y', which='major', labelsize=8,length = 1,color = 'black',width =10)\n",
        "\n",
        "  # Set title\n",
        "  title = plt.title(plot_title,fontweight ='bold',size=10, pad=20, wrap=False)\n",
        "  title.set_position([.35, 1])\n",
        "\n",
        "  # Set subtitle\n",
        "  tform = ax.get_xaxis_transform()\n",
        "\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.spines['left'].set_visible(True)\n",
        "  ax.spines['bottom'].set_visible(True)\n",
        "\n",
        "  plt.savefig(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{FC_sex}/String_Ontology_Stats/{ontology_category}_{comparison}_RnC_Plot.svg',format=\"svg\",transparent=True,bbox_inches='tight')\n",
        "\n",
        "\n",
        "for ontology_category,ontologies in List_Dictionary_E2vE3.items():\n",
        "  print(ontology_category)\n",
        "  bar_plot_for_ontologies(input_df=RnC_Df_for_lists_E2vE3,\n",
        "                          list_of_interest=ontologies,\n",
        "                          Ontology_name=ontology_category,comparison='E2vE3')\n",
        "\n",
        "for ontology_category,ontologies in List_Dictionary_E4vE3.items():\n",
        "  print(ontology_category,ontologies)\n",
        "  bar_plot_for_ontologies(input_df=RnC_Df_for_lists_E4vE3,\n",
        "                          list_of_interest=ontologies,\n",
        "                          Ontology_name=ontology_category,comparison='E4vE3')"
      ],
      "metadata": {
        "id": "m1765X2Sk5TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ont_Dictionary = {'Mitochondrial Components':['GO:0031966','GO:0005743','GO:1990542','GO:0098798','GO:0030150','GO:0005759','GO:0031966','GO:0098573','GO:0031305','GO:0005742','GO:0098800'],\n",
        "                         'OXPHOS':['MMU-1428517','GO:0022900','GO:0033177','MMU-204174','GO:0005967','mmu00020','GO:0022904','MMU-1428517','GO:0009060','mmu00051'],\n",
        "                         'ProteasomeLysosome':['GO:0005839','GO:0030544','GO:0008541','GO:0005838','GO:0022624','GO:0000502'],\n",
        "                          'Endolysosomal Pathway':['GO:0005765','GO:0005764','MMU-8856828','GO:0005770','GO:0031902','GO:0007041','GO:0007040']\n",
        "                         }\n",
        "\n",
        "def Ontology_protein_dictionary(SDB_conc_Stats=None,comparison=None):\n",
        "  if comparison == 'E2vE3':\n",
        "    List_Dictionary = ont_Dictionary\n",
        "  elif comparison == 'E4vE3':\n",
        "    List_Dictionary = ont_Dictionary\n",
        "\n",
        "  SDB_conc_Stats = SDB_conc_Stats.reset_index()\n",
        "  Ontology_protein_dictionary = {}\n",
        "\n",
        "  for ontology_category,ontologies in List_Dictionary.items():\n",
        "\n",
        "    shared_counts_df = pd.DataFrame(None)\n",
        "    Ontology_protein_dictionary = {}\n",
        "\n",
        "    for ontology in ontologies:\n",
        "      ontology_name = SDB_conc_Stats.loc[SDB_conc_Stats['term ID'] == ontology,'term description (Count)'].values.tolist()[0]\n",
        "      ontology_proteins = SDB_conc_Stats.loc[SDB_conc_Stats['term ID'] == ontology,f'matching proteins in your network (labels)'].values.tolist()[0]\n",
        "      ontology_proteins = ontology_proteins[:].strip(r'[|]')\n",
        "      ontology_proteins = ontology_proteins[:].replace(\" \", \"\")\n",
        "      ontology_proteins = ontology_proteins[:].replace(\"'\", \"\")\n",
        "      ontology_proteins = ontology_proteins[:].split(r',')\n",
        "      Ontology_protein_dictionary[f'{ontology_name}'] = ontology_proteins\n",
        "\n",
        "    protein_lists = list(Ontology_protein_dictionary.values())\n",
        "    ontology_list = list(Ontology_protein_dictionary.keys())\n",
        "\n",
        "    def calculate_shared_percentage(list_dict):\n",
        "        list_names = list(list_dict.keys())\n",
        "        shared_percentages = {}\n",
        "        for i in range(len(list_names)):\n",
        "\n",
        "            for j in range(i + 1, len(list_names)):\n",
        "                list1_set = set(list_dict[list_names[i]])\n",
        "                list2_set = set(list_dict[list_names[j]])\n",
        "                shared_items = len(list1_set & list2_set)\n",
        "                total_items = len(list1_set | list2_set)\n",
        "                shared_percentages[(i, j)] = (shared_items / total_items) * 100\n",
        "            shared_percentages[(i, i)] = 100\n",
        "\n",
        "        return shared_percentages\n",
        "\n",
        "    def create_heatmap(shared_percentages, list_names):\n",
        "        matrix = [[0 for _ in range(len(list_names))] for _ in range(len(list_names))]\n",
        "        for (i, j), percentage in shared_percentages.items():\n",
        "            matrix[i][j] = percentage\n",
        "            matrix[j][i] = percentage\n",
        "        return matrix\n",
        "\n",
        "\n",
        "    shared_percentages = calculate_shared_percentage(Ontology_protein_dictionary)\n",
        "    list_names = list(Ontology_protein_dictionary.keys())\n",
        "\n",
        "    # Create the heatmap data\n",
        "    heatmap_data = create_heatmap(shared_percentages, list_names)\n",
        "    heatmap_df = pd.DataFrame(heatmap_data, index=list_names, columns=list_names)\n",
        "\n",
        "    labels = heatmap_df.index\n",
        "    labels = [ '\\n'.join(wrap(l, 35)) for l in labels ]\n",
        "\n",
        "    sns.heatmap(heatmap_df, annot=True, cmap='crest', fmt='.0f', linewidths = 0.30, robust=True,label=labels,xticklabels=labels,yticklabels=labels)\n",
        "\n",
        "    # plt.xlabel('Ontology', fontweight ='bold', size=20)\n",
        "    # plt.ylabel('Ontology', fontweight ='bold', size=20)\n",
        "    plt.title('Ontology Shared Proteins (%)',fontweight ='bold',size=10, pad=15, wrap=False)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=10,length = 5,color = 'black',width =3)\n",
        "    plt.savefig(f'/content/drive/MyDrive/ApoE Analysis January-11-2023/{FC_sex}/String_Ontology_Stats/{ontology_category}_HEATMAP_Plot.svg',format=\"svg\",transparent=True,bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "Ontology_protein_dictionary_E2vE3 = Ontology_protein_dictionary(SDB_conc_Stats=RnC_Df_for_lists_E2vE3,comparison='E2vE3')"
      ],
      "metadata": {
        "id": "IH6u2H4ylGQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}